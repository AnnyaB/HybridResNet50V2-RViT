# dataset_prep.py
# I started working on dataset preparation from 12th november 2025

# -------------------------------------------------------------------
# dataset preparation pipeline (what I already did)
#
# In this script I had:
#   - walked the original Kaggle brain tumour MRI folders and recorded
#     every file path together with its class and Kaggle split
#     (Training / Testing),
#   - computed SHA1 hashes to detect and remove exact byte-for-byte
#     duplicate files, while also saving a detailed duplicate summary,
#   - audited every deduplicated raw image for geometry and intensity:
#       – width, height, aspect ratio,
#       – mean / std / min / max grayscale intensity,
#       – conservative flags for too dark, too bright, low contrast,
#         and a combined "suspect" quality flag,
#   - produced CSV summaries for raw resolutions and quality flags so
#     that I could describe the original dataset objectively in the
#     report,
#   - created a *leakage-safe* split aligned to Kaggle's intended protocol:
#       - Kaggle Training -> stratified Train/Val
#       - Kaggle Testing  -> held-out Test (never used for training)
#   - defined a tight brain-cropping function that removed black
#     background based on an intensity threshold plus a small margin,
#   - applied this crop to every image in the split, resized to
#     224x224 (matching ImageNet / ResNet50V2 / RViT defaults),
#     converted to RGB, and saved them into a clean
#     data/processed/tightcrop/{split}/{class}/ structure,
#   - saved train/val/test CSVs that point to these cropped 224×224
#     images (for model training and evaluation),
#   - saved an overall split summary CSV with per-class counts per
#     split, which I later used to generate figures in dataset_plots.py.
#
# This file therefore gave me a single, reproducible, auditable
# pipeline from raw Kaggle folders -> cleaned, cropped 224x224 dataset
# and analysis artefacts for the project's methodology section.
# -------------------------------------------------------------------

"""
Dataset preparation script for HybridResNet50V2–RViT brain tumour classification.

This file builds one reproducible pipeline that:

  - walks the raw Kaggle folders and records exactly what is there,
  - removes byte-for-byte duplicate files using SHA1 with a leakage-safe policy
    (prefer keeping Kaggle Testing copies when duplicates exist across splits),
  - audits raw images for geometry and intensity problems
    (so we can describe “suspect” images with evidence),
  - creates a Kaggle-aligned evaluation protocol:
      * Kaggle Training -> stratified Train/Val split (default 80/20),
      * Kaggle Testing  -> held-out Test set,
  - and finally writes *cropped* 224×224 RGB images and CSVs for model training.

Important design decisions:

  - All model training uses only the cropped 224×224 images.
  - Cropping is done first (tight crop around brain region), then resized to 224×224.
  - The audit (intensity, duplicates) is always done on the raw
    uncropped images so we can describe the original dataset properly.
  - Exact duplicates are removed BEFORE splitting, and duplicates that cross
    Training/Testing are resolved by keeping the Testing copy to avoid leakage.
"""

import os
from pathlib import Path
import hashlib
import math

import numpy as np
from PIL import Image, ImageOps
from sklearn.model_selection import train_test_split
import pandas as pd
from tqdm import tqdm


# -------------------------------------------------------------------
# CONFIGURATION SECTION
# -------------------------------------------------------------------

PROJECT_ROOT = Path(__file__).resolve().parents[1]

RAW_ROOT = (
    PROJECT_ROOT
    / "data"
    / "raw"
    / "brain-tumor-mri-dataset"
    / "kaggle_brain_mri_scan_dataset"
)

PROCESSED_BASE = PROJECT_ROOT / "data" / "processed"
SPLITS_BASE = PROJECT_ROOT / "data" / "splits"
RESULTS_DIR = PROJECT_ROOT / "results"

VARIANT = "tightcrop"
PROCESSED_ROOT = PROCESSED_BASE / VARIANT
SPLITS_ROOT = SPLITS_BASE / VARIANT

SUMMARY_PATH                 = RESULTS_DIR / "dataset_summary.csv"
RAW_STATS_PATH               = RESULTS_DIR / "raw_image_stats.csv"
RAW_RESOLUTION_SUMMARY_PATH  = RESULTS_DIR / "raw_resolution_summary.csv"
RAW_QUALITY_SUMMARY_PATH     = RESULTS_DIR / "raw_quality_flags_summary.csv"
RAW_CLASS_COUNTS_PATH        = RESULTS_DIR / "raw_class_counts_by_source.csv"
DUPLICATES_PATH              = RESULTS_DIR / "duplicate_files.csv"
DUPLICATE_SUMMARY_PATH       = RESULTS_DIR / "duplicate_summary.csv"

IMG_SIZE = (224, 224)
CLASSES = ["glioma", "meningioma", "pituitary", "notumor"]
RANDOM_STATE = 42

# Kaggle-aligned split:
# - Training -> Train/Val
# - Testing  -> Test (held-out)
VAL_FRACTION_OF_TRAINING = 0.20   # 80/20 split inside Kaggle Training

# Dedup policy:
# If an exact duplicate exists in BOTH Training and Testing, keep Testing copy.
PREFER_TESTING_ON_CROSS_SPLIT_DUPLICATES = True

BACKGROUND_INTENSITY_THRESHOLD = 5
CROP_MARGIN = 10

# Only process real image files (prevents .DS_Store, Thumbs.db, etc.)
ALLOWED_EXTS = {".jpg", ".jpeg", ".png", ".bmp", ".tif", ".tiff"}


def _get_resample():
    """
    Pillow version compatibility: Image.Resampling exists in newer versions.
    We pick LANCZOS for best downsampling quality.
    """
    try:
        return Image.Resampling.LANCZOS
    except AttributeError:
        return Image.LANCZOS


RESAMPLE = _get_resample()


# -------------------------------------------------------------------
# COLLECTING RAW IMAGE PATHS
# -------------------------------------------------------------------

def collect_images() -> pd.DataFrame:
    """
    Walk through the raw Kaggle folders and build a DataFrame.

    For each file record:
      - orig_path    : full path on disk
      - class        : glioma / meningioma / pituitary / notumor
      - source_split : "training" or "testing" (Kaggle's folder)
    """
    records = []

    for split in ["Training", "Testing"]:
        for cls in CLASSES:
            folder = RAW_ROOT / split / cls
            if not folder.exists():
                continue

            for p in folder.iterdir():
                if not p.is_file():
                    continue

                # Skip hidden/system files and non-image extensions
                if p.name.startswith("."):
                    continue
                if p.suffix.lower() not in ALLOWED_EXTS:
                    continue

                records.append(
                    {
                        "orig_path": str(p),
                        "class": cls,
                        "source_split": split.lower(),  # "training" / "testing"
                    }
                )

    df = pd.DataFrame(records)
    print(f"Total images found (before deduplication): {len(df)}")
    return df


def save_raw_class_counts(df: pd.DataFrame):
    """
    Summarise how many images there are per class in Kaggle's
    original Training vs Testing folders.

    Output: results/raw_class_counts_by_source.csv
    """
    RAW_CLASS_COUNTS_PATH.parent.mkdir(parents=True, exist_ok=True)

    if df.empty:
        print("No images found; raw class counts not saved.")
        return

    counts = (
        df.groupby(["source_split", "class"])
        .size()
        .reset_index(name="count")
        .sort_values(["source_split", "class"])
    )

    counts.to_csv(RAW_CLASS_COUNTS_PATH, index=False)
    print(f"Saved raw class counts (by Kaggle split) to {RAW_CLASS_COUNTS_PATH}")
    print(counts)


# -------------------------------------------------------------------
# EXACT DUPLICATE REMOVAL (SHA1) - LEAKAGE SAFE
# -------------------------------------------------------------------

def sha1_of_file(path: str, block_size: int = 65536) -> str:
    """
    Compute SHA1 hash of a file.
    If two files have the same SHA1, they are byte-for-byte identical.
    """
    h = hashlib.sha1()
    with open(path, "rb") as f:
        for chunk in iter(lambda: f.read(block_size), b""):
            h.update(chunk)
    return h.hexdigest()

def processed_filename_for(orig_path: str) -> str:
    """
    Build a unique, deterministic filename for the processed image
    to avoid collisions (same src.name in different folders).

    We use SHA1(file_bytes) + original extension.
    """
    p = Path(orig_path)
    return f"{sha1_of_file(str(p))}{p.suffix.lower()}"


def drop_duplicates_leakage_safe(df: pd.DataFrame):
    """
    Remove exact duplicate files based on SHA1, using a leakage-safe policy.

    Key rule:
      - If a duplicate group spans BOTH Kaggle Training and Kaggle Testing,
        we KEEP the Testing copy and DROP the Training copy(s).

    Returns:
      - dedup_df : DataFrame of kept rows only (unique SHA1)
      - dups_df  : DataFrame of all rows that belong to duplicate groups,
                  including which file was kept/dropped
    """
    if df.empty:
        return df.copy(), pd.DataFrame()

    print("Computing SHA1 hashes for duplicate detection...")
    df = df.copy()
    df["sha1"] = df["orig_path"].apply(sha1_of_file)

    before = len(df)

    # Identify duplicate groups (size >= 2)
    dup_mask = df.duplicated(subset="sha1", keep=False)
    dups_df = df[dup_mask].copy()

    if dups_df.empty:
        print("No duplicate files detected based on SHA1 hashes.")
        dedup_df = df.reset_index(drop=True)
        print(f"Unique files after deduplication: {len(dedup_df)}")
        return dedup_df, dups_df

    keep_indices = []

    # For deterministic behaviour, we sort paths inside each group
    for sha1, group in dups_df.groupby("sha1"):
        g = group.sort_values(["source_split", "orig_path"]).copy()

        has_testing = (g["source_split"] == "testing").any()
        has_training = (g["source_split"] == "training").any()

        # Leakage-safe decision:
        if PREFER_TESTING_ON_CROSS_SPLIT_DUPLICATES and has_testing and has_training:
            # Keep ONE testing copy (deterministic: lexicographically smallest testing path)
            keep_row = g[g["source_split"] == "testing"].sort_values("orig_path").iloc[0]
        else:
            # Otherwise keep ONE file deterministically (smallest path)
            keep_row = g.sort_values("orig_path").iloc[0]

        keep_indices.append(int(keep_row.name))

    # Keep all non-duplicates + one representative from each duplicate group
    non_dup_df = df[~dup_mask].copy()
    kept_from_dups_df = df.loc[keep_indices].copy()

    dedup_df = pd.concat([non_dup_df, kept_from_dups_df], axis=0).reset_index(drop=True)

    after = len(dedup_df)
    print(f"Unique files after deduplication: {after}")
    print(f"Removed {before - after} duplicate file entries (if any).")

    # Augment dups_df with keep/drop info for auditing
    dups_df = dups_df.copy()
    dups_df["kept"] = False

    # Mark kept rows inside duplicate groups
    kept_set = set(keep_indices)
    dups_df.loc[dups_df.index.isin(kept_set), "kept"] = True

    # Also include which path was kept in the group (helpful for report/audit)
    kept_path_by_sha1 = (
        dups_df[dups_df["kept"]]
        .set_index("sha1")["orig_path"]
        .to_dict()
    )
    dups_df["kept_path_for_group"] = dups_df["sha1"].map(kept_path_by_sha1)

    return dedup_df, dups_df


def save_duplicate_summary(dups_df: pd.DataFrame):
    """
    Write detailed information about exact duplicates:

      - duplicate_files.csv   : full listing of all duplicate-group members,
                               including which one was kept
      - duplicate_summary.csv : one row per SHA1 group
    """
    DUPLICATES_PATH.parent.mkdir(parents=True, exist_ok=True)

    if dups_df is None or dups_df.empty:
        print("No duplicate files to save.")
        return

    dups_df.to_csv(DUPLICATES_PATH, index=False)
    print(f"Saved full duplicate listing to {DUPLICATES_PATH}")

    rows = []
    for sha1, group in dups_df.groupby("sha1"):
        group = group.copy()

        kept_rows = group[group["kept"]]
        kept_path = kept_rows["orig_path"].iloc[0] if not kept_rows.empty else ""

        # Helpful flags: duplicates crossing splits, or (rare) crossing classes
        crosses_splits = group["source_split"].nunique() > 1
        crosses_classes = group["class"].nunique() > 1

        rows.append(
            {
                "sha1": sha1,
                "n_files": len(group),
                "crosses_splits": bool(crosses_splits),
                "crosses_classes": bool(crosses_classes),
                "classes": ";".join(sorted(group["class"].unique())),
                "source_splits": ";".join(sorted(group["source_split"].unique())),
                "kept_path": kept_path,
                "dropped_paths_example": "; ".join(group[~group["kept"]]["orig_path"].head(5)),
            }
        )

    summary_df = pd.DataFrame(rows).sort_values("n_files", ascending=False)
    summary_df.to_csv(DUPLICATE_SUMMARY_PATH, index=False)
    print(f"Saved duplicate summary to {DUPLICATE_SUMMARY_PATH}")
    print(summary_df.head())


# -------------------------------------------------------------------
# RAW IMAGE ANALYSIS (GEOMETRY and INTENSITY)
# -------------------------------------------------------------------

def analyze_raw_images(df: pd.DataFrame) -> pd.DataFrame:
    """
    Audit each deduplicated raw image before resizing.

    For each image record:
      - width, height, aspect_ratio
      - grayscale intensity stats: mean, std, min, max
      - a 'failed' flag if PIL couldn't read the file
      - conservative quality flags:
          too_dark, too_bright, low_contrast, suspect

    This function does NOT drop images; it only flags them.
    """
    records = []
    print("Analysing raw image geometry and intensity statistics...")

    for row in tqdm(df.to_dict(orient="records"), desc="Analysing raw images"):
        path = row["orig_path"]
        cls = row["class"]
        source_split = row["source_split"]
        sha1 = row.get("sha1", None)

        width = height = None
        aspect_ratio = None
        mean_intensity = std_intensity = None
        min_intensity = max_intensity = None
        failed = False

        try:
            img = Image.open(path)
            img = ImageOps.exif_transpose(img)
            img.load()  # force decode to catch corrupt images

            width, height = img.size
            aspect_ratio = (width / height) if height else np.nan

            gray = img.convert("L")
            arr = np.array(gray, dtype=np.float32)

            mean_intensity = float(arr.mean())
            std_intensity = float(arr.std())
            min_intensity = float(arr.min())
            max_intensity = float(arr.max())

        except Exception as e:
            print(f"Failed to analyse {path}: {e}")
            failed = True

        records.append(
            {
                "orig_path": path,
                "class": cls,
                "source_split": source_split,
                "sha1": sha1,
                "width": width,
                "height": height,
                "aspect_ratio": aspect_ratio,
                "mean_intensity": mean_intensity,
                "std_intensity": std_intensity,
                "min_intensity": min_intensity,
                "max_intensity": max_intensity,
                "failed": failed,
            }
        )

    stats_df = pd.DataFrame(records)
    not_failed = ~stats_df["failed"]

    stats_df["too_dark"] = (
        not_failed
        & (stats_df["mean_intensity"] < 15)
        & (stats_df["max_intensity"] < 60)
    )

    stats_df["too_bright"] = (
        not_failed
        & (stats_df["mean_intensity"] > 240)
        & (stats_df["min_intensity"] > 200)
    )

    stats_df["low_contrast"] = not_failed & (stats_df["std_intensity"] < 5)

    stats_df["suspect"] = (
        stats_df["too_dark"]
        | stats_df["too_bright"]
        | stats_df["low_contrast"]
        | stats_df["failed"]
    )

    return stats_df


def save_raw_analysis(stats_df: pd.DataFrame):
    """
    Save raw-image analysis to:
      - raw_image_stats.csv
      - raw_resolution_summary.csv
      - raw_quality_flags_summary.csv
    """
    RAW_STATS_PATH.parent.mkdir(parents=True, exist_ok=True)

    if stats_df.empty:
        print("No stats to save; stats_df is empty.")
        return

    stats_df.to_csv(RAW_STATS_PATH, index=False)
    print(f"Saved raw image stats to {RAW_STATS_PATH}")

    res_summary = (
        stats_df.groupby(["width", "height"])
        .size()
        .reset_index(name="count")
        .sort_values("count", ascending=False)
    )
    res_summary.to_csv(RAW_RESOLUTION_SUMMARY_PATH, index=False)
    print(f"Saved resolution summary to {RAW_RESOLUTION_SUMMARY_PATH}")
    print("Top 5 most common resolutions:")
    print(res_summary.head())

    qual_summary = (
        stats_df.groupby("class")[["too_dark", "too_bright", "low_contrast", "suspect", "failed"]]
        .sum()
        .reset_index()
    )
    qual_summary.to_csv(RAW_QUALITY_SUMMARY_PATH, index=False)
    print(f"Saved quality flags summary to {RAW_QUALITY_SUMMARY_PATH}")
    print(qual_summary)

    total_suspect = int(stats_df["suspect"].sum())
    print(f"Total suspect images (any flag or failed): {total_suspect}")


# -------------------------------------------------------------------
# TIGHT BRAIN CROPPING - KAGGLE-ALIGNED SPLITS - RESIZED IMAGES
# -------------------------------------------------------------------

def tight_crop_to_brain(img: Image.Image) -> Image.Image:
    """
    Crop away black background around the brain using a simple intensity mask.
    If no foreground pixels are found, return the original image.
    """
    gray = img.convert("L")
    arr = np.array(gray, dtype=np.uint8)

    mask = arr > BACKGROUND_INTENSITY_THRESHOLD
    if not mask.any():
        return img

    ys, xs = np.where(mask)

    y_min = max(int(ys.min()) - CROP_MARGIN, 0)
    y_max = min(int(ys.max()) + 1 + CROP_MARGIN, arr.shape[0])

    x_min = max(int(xs.min()) - CROP_MARGIN, 0)
    x_max = min(int(xs.max()) + 1 + CROP_MARGIN, arr.shape[1])

    return img.crop((x_min, y_min, x_max, y_max))


def make_splits_kaggle_aligned(df: pd.DataFrame):
    """
    Create leakage-safe, Kaggle-aligned splits:

      - Train/Val are created ONLY from Kaggle Training images (stratified).
      - Test is the Kaggle Testing folder (held-out, no splitting).

    This is the correct evaluation setup for this Kaggle dataset:
    we do NOT create a new test set from Training because Kaggle already provides one.
    """
    if df.empty:
        raise ValueError("Cannot split an empty dataset.")

    df_train_source = df[df["source_split"] == "training"].copy()
    df_test_source  = df[df["source_split"] == "testing"].copy()

    if df_train_source.empty:
        raise ValueError("No Kaggle Training images found after deduplication.")
    if df_test_source.empty:
        print("WARNING: No Kaggle Testing images found after deduplication. Test set will be empty.")

    X = df_train_source["orig_path"].values
    y = df_train_source["class"].values

    # Stratified train/val split inside Kaggle Training only
    try:
        X_train, X_val, y_train, y_val = train_test_split(
            X,
            y,
            test_size=VAL_FRACTION_OF_TRAINING,
            stratify=y,
            random_state=RANDOM_STATE,
        )
    except Exception as e:
        # Fallback: non-stratified split (should not happen with this dataset, but keeps script robust)
        print(f"WARNING: Stratified split failed ({e}). Falling back to non-stratified split.")
        X_train, X_val, y_train, y_val = train_test_split(
            X,
            y,
            test_size=VAL_FRACTION_OF_TRAINING,
            random_state=RANDOM_STATE,
        )

    def to_df(paths, labels, split_name: str) -> pd.DataFrame:
        return pd.DataFrame(
            {
                "orig_path": [str(Path(p)) for p in paths],
                "class": labels,
                "split": split_name,
            }
        )

    df_train = to_df(X_train, y_train, "train")
    df_val   = to_df(X_val, y_val, "val")

    # Kaggle Testing is held-out test
    df_test = pd.DataFrame(
        {
            "orig_path": df_test_source["orig_path"].astype(str).values,
            "class": df_test_source["class"].values,
            "split": "test",
        }
    )

    print("Split sizes (Kaggle-aligned, after deduplication):")
    print(f"  Train (from Kaggle Training): {len(df_train)}")
    print(f"  Val   (from Kaggle Training): {len(df_val)}")
    print(f"  Test  (Kaggle Testing):       {len(df_test)}")

    return df_train, df_val, df_test


def prepare_processed_dirs():
    """
    Ensure canonical directory structure exists for the cropped variant:

        data/processed/tightcrop/train/{class}/
        data/processed/tightcrop/val/{class}/
        data/processed/tightcrop/test/{class}/
    """
    for split in ["train", "val", "test"]:
        for cls in CLASSES:
            out_dir = PROCESSED_ROOT / split / cls
            out_dir.mkdir(parents=True, exist_ok=True)


def resize_and_copy(df_split: pd.DataFrame, split_name: str):
    """
    Create the processed dataset:

      - read raw image
      - exif_transpose (safety)
      - tight crop around brain
      - resize to IMG_SIZE (224x224) using LANCZOS
      - convert to RGB
      - save into data/processed/tightcrop/{split}/{class}/

    NOTE: Files are saved using SHA1-based filenames to prevent collisions.
    """
    rows = df_split.to_dict(orient="records")

    for row in tqdm(rows, desc=f"Processing {split_name} [{VARIANT}]"):
        src = Path(row["orig_path"])
        cls = row["class"]

        out_name = processed_filename_for(row["orig_path"])
        dst = PROCESSED_ROOT / split_name / cls / out_name

        if dst.exists():
            continue

        try:
            img = Image.open(src)
            img = ImageOps.exif_transpose(img)
            img = img.convert("RGB")

            img = tight_crop_to_brain(img)
            img = img.resize(IMG_SIZE, resample=RESAMPLE)

            img.save(dst)
        except Exception as e:
            print(f"Failed to process {src}: {e}")



def save_csv_splits(df_train, df_val, df_test):
    """
    Build CSVs that map to the *processed* (cropped) paths.

    Each CSV has columns:
      - image_path : path to cropped 224x224 RGB image
      - class      : tumour class label
    """
    SPLITS_ROOT.mkdir(parents=True, exist_ok=True)

    def map_to_processed(df: pd.DataFrame, split_name: str) -> pd.DataFrame:
        processed_paths = []
        for _, row in df.iterrows():
            cls = row["class"]
            out_name = processed_filename_for(row["orig_path"])
            processed_paths.append(str(PROCESSED_ROOT / split_name / cls / out_name))
        return pd.DataFrame({"image_path": processed_paths, "class": df["class"].values})

    train_csv = map_to_processed(df_train, "train")
    val_csv   = map_to_processed(df_val, "val")
    test_csv  = map_to_processed(df_test, "test")

    train_csv.to_csv(SPLITS_ROOT / "train.csv", index=False)
    val_csv.to_csv(SPLITS_ROOT / "val.csv", index=False)
    test_csv.to_csv(SPLITS_ROOT / "test.csv", index=False)

    print(f"Saved split CSVs to {SPLITS_ROOT}.")


def save_summary(df_train, df_val, df_test):
    """
    Summarise class counts per split.

    Output:
      - results/dataset_summary.csv
    """
    SUMMARY_PATH.parent.mkdir(parents=True, exist_ok=True)

    def counts(df, split_name):
        c = df["class"].value_counts().rename("count").reset_index()
        c = c.rename(columns={"index": "class"})
        c.insert(0, "split", split_name)
        return c

    summary_df = pd.concat(
        [counts(df_train, "train"), counts(df_val, "val"), counts(df_test, "test")],
        axis=0,
        ignore_index=True,
    )

    summary_df.to_csv(SUMMARY_PATH, index=False)
    print("Saved dataset summary to results/dataset_summary.csv")
    print(summary_df)


# -------------------------------------------------------------------
# MAIN ORCHESTRATION
# -------------------------------------------------------------------

def main():
    """
    Orchestrate the entire dataset preparation pipeline:

      1. Collect raw images from Kaggle folders
      2. Save raw class counts per Kaggle split
      3. Deduplicate based on SHA1 with leakage-safe policy (prefer keep Testing on cross-split dups)
      4. Analyse raw images (geometry + intensity + quality flags)
      5. Save analysis CSVs
      6. Create Kaggle-aligned splits:
           - Train/Val from Kaggle Training (stratified)
           - Test from Kaggle Testing (held-out)
      7. Write cropped 224x224 RGB images into data/processed/tightcrop/
      8. Save train/val/test CSVs and overall summary table
    """
    print(f"Running dataset preparation for VARIANT = '{VARIANT}' (cropped-only pipeline)")

    # Step 1: collect raw image paths
    df_raw = collect_images()

    # Step 2: raw class counts per Kaggle split
    save_raw_class_counts(df_raw)

    # Step 3: exact duplicates (SHA1) with leakage-safe policy
    df_dedup, dups_df = drop_duplicates_leakage_safe(df_raw)
    save_duplicate_summary(dups_df)

    # Step 4: analyse raw images
    stats_df = analyze_raw_images(df_dedup)

    # Step 5: save analysis artefacts
    save_raw_analysis(stats_df)

    # Step 6: Kaggle-aligned splits (paper/evaluation correct)
    df_train, df_val, df_test = make_splits_kaggle_aligned(df_dedup)

    # Step 7: prepare dirs + write processed images
    prepare_processed_dirs()
    resize_and_copy(df_train, "train")
    resize_and_copy(df_val, "val")
    resize_and_copy(df_test, "test")

    # Step 8: save split CSVs + summary
    save_csv_splits(df_train, df_val, df_test)
    save_summary(df_train, df_val, df_test)

    print("Dataset preparation and audit completed (cropped 224x224 images only).")


if __name__ == "__main__":
    main()
