{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62848fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(\"scripts\", exist_ok=True)\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "os.makedirs(\"data/splits/tightcrop\", exist_ok=True)\n",
    "os.makedirs(\"results\", exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323c76ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile scripts/data.py\n",
    "# scripts/data.py\n",
    "#\n",
    "# Dataset loader for CSV splits produced by dataset_prep.py:\n",
    "#   data/splits/tightcrop/{train,val,test}.csv\n",
    "# CSV columns:\n",
    "#   - image_path\n",
    "#   - class  (glioma/meningioma/pituitary/notumor)\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "class AddGaussianNoise:\n",
    "    def __init__(self, std=0.02, p=0.5):\n",
    "        self.std = std\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if torch.rand(1).item() > self.p:\n",
    "            return x\n",
    "        noise = torch.randn_like(x) * self.std\n",
    "        x = x + noise\n",
    "        return torch.clamp(x, 0.0, 1.0)\n",
    "\n",
    "\n",
    "def _resolve_path(p, project_root):\n",
    "    # Your CSV may contain absolute paths from a different machine.\n",
    "    # This resolver makes the project portable for Colab/Kaggle.\n",
    "    if os.path.exists(p):\n",
    "        return p\n",
    "\n",
    "    p2 = str(p).replace(\"\\\\\", \"/\")\n",
    "    key = \"/data/processed/\"\n",
    "    idx = p2.find(key)\n",
    "    if idx != -1:\n",
    "        rel = p2[idx + 1:]  # remove leading '/'\n",
    "        cand = str(Path(project_root) / rel)\n",
    "        if os.path.exists(cand):\n",
    "            return cand\n",
    "\n",
    "    # fallback: try relative to project_root directly\n",
    "    cand2 = str(Path(project_root) / p2)\n",
    "    if os.path.exists(cand2):\n",
    "        return cand2\n",
    "\n",
    "    return p  # let it fail loudly in __getitem__ for debugging\n",
    "\n",
    "\n",
    "def build_transforms(train, mean, std):\n",
    "    ops = []\n",
    "    if train:\n",
    "        ops.extend([\n",
    "            T.RandomRotation(degrees=15),\n",
    "            T.RandomHorizontalFlip(p=0.5),\n",
    "            T.RandomAffine(degrees=0, translate=(0.05, 0.05)),\n",
    "        ])\n",
    "\n",
    "    ops.extend([T.ToTensor()])\n",
    "\n",
    "    if train:\n",
    "        ops.append(AddGaussianNoise(std=0.02, p=0.5))\n",
    "\n",
    "    ops.append(T.Normalize(mean=mean, std=std))\n",
    "    return T.Compose(ops)\n",
    "\n",
    "\n",
    "class BrainMRICSV(Dataset):\n",
    "    def __init__(self, csv_path, class_names, transform, project_root):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.class_names = class_names\n",
    "        self.class_to_idx = {c: i for i, c in enumerate(class_names)}\n",
    "        self.transform = transform\n",
    "        self.project_root = project_root\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = _resolve_path(row[\"image_path\"], self.project_root)\n",
    "        y_str = row[\"class\"]\n",
    "        y = self.class_to_idx[y_str]\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        x = self.transform(img)\n",
    "        return x, y, img_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfd857f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile scripts/dataset_prep.py\n",
    "# dataset_prep.py\n",
    "# I started working on dataset preparation from 12th november 2025\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# dataset preparation pipeline (what I already did)\n",
    "#\n",
    "# In this script I had:\n",
    "#   - walked the original Kaggle brain tumour MRI folders and recorded\n",
    "#     every file path together with its class and Kaggle split\n",
    "#     (Training / Testing),\n",
    "#   - computed SHA1 hashes to detect and remove exact byte-for-byte\n",
    "#     duplicate files, while also saving a detailed duplicate summary,\n",
    "#   - audited every deduplicated raw image for geometry and intensity:\n",
    "#       â€“ width, height, aspect ratio,\n",
    "#       â€“ mean / std / min / max grayscale intensity,\n",
    "#       â€“ conservative flags for too dark, too bright, low contrast,\n",
    "#         and a combined \"suspect\" quality flag,\n",
    "#   - produced CSV summaries for raw resolutions and quality flags so\n",
    "#     that I could describe the original dataset objectively in the\n",
    "#     report,\n",
    "#   - created a *leakage-safe* split aligned to Kaggle's intended protocol:\n",
    "#       - Kaggle Training -> stratified Train/Val\n",
    "#       - Kaggle Testing  -> held-out Test (never used for training)\n",
    "#   - defined a tight brain-cropping function that removed black\n",
    "#     background based on an intensity threshold plus a small margin,\n",
    "#   - applied this crop to every image in the split, resized to\n",
    "#     224x224 (matching ImageNet / ResNet50V2 / RViT defaults),\n",
    "#     converted to RGB, and saved them into a clean\n",
    "#     data/processed/tightcrop/{split}/{class}/ structure,\n",
    "#   - saved train/val/test CSVs that point to these cropped 224Ã—224\n",
    "#     images (for model training and evaluation),\n",
    "#   - saved an overall split summary CSV with per-class counts per\n",
    "#     split, which I later used to generate figures in dataset_plots.py.\n",
    "#\n",
    "# This file therefore gave me a single, reproducible, auditable\n",
    "# pipeline from raw Kaggle folders -> cleaned, cropped 224x224 dataset\n",
    "# and analysis artefacts for the project's methodology section.\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "\"\"\"\n",
    "Dataset preparation script for HybridResNet50V2â€“RViT brain tumour classification.\n",
    "\n",
    "This file builds one reproducible pipeline that:\n",
    "\n",
    "  - walks the raw Kaggle folders and records exactly what is there,\n",
    "  - removes byte-for-byte duplicate files using SHA1 with a leakage-safe policy\n",
    "    (prefer keeping Kaggle Testing copies when duplicates exist across splits),\n",
    "  - audits raw images for geometry and intensity problems\n",
    "    (so we can describe â€œsuspectâ€ images with evidence),\n",
    "  - creates a Kaggle-aligned evaluation protocol:\n",
    "      * Kaggle Training -> stratified Train/Val split (default 80/20),\n",
    "      * Kaggle Testing  -> held-out Test set,\n",
    "  - and finally writes *cropped* 224Ã—224 RGB images and CSVs for model training.\n",
    "\n",
    "Important design decisions:\n",
    "\n",
    "  - All model training uses only the cropped 224Ã—224 images.\n",
    "  - Cropping is done first (tight crop around brain region), then resized to 224Ã—224.\n",
    "  - The audit (intensity, duplicates) is always done on the raw\n",
    "    uncropped images so we can describe the original dataset properly.\n",
    "  - Exact duplicates are removed BEFORE splitting, and duplicates that cross\n",
    "    Training/Testing are resolved by keeping the Testing copy to avoid leakage.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import hashlib\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# CONFIGURATION SECTION\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Kaggle paths\n",
    "RAW_DATA_DIR = Path(\"/kaggle/input/btc-dataset\")\n",
    "PROJECT_ROOT = Path(\"/kaggle/working\")\n",
    "\n",
    "# ðŸ”‘ FIX: point to actual dataset root\n",
    "RAW_ROOT = RAW_DATA_DIR / \"kaggle_brain_mri_scan_dataset\"\n",
    "\n",
    "PROCESSED_BASE = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "SPLITS_BASE = PROJECT_ROOT / \"data\" / \"splits\"\n",
    "RESULTS_DIR = PROJECT_ROOT / \"results\"\n",
    "\n",
    "VARIANT = \"tightcrop\"\n",
    "PROCESSED_ROOT = PROCESSED_BASE / VARIANT\n",
    "SPLITS_ROOT = SPLITS_BASE / VARIANT\n",
    "\n",
    "SUMMARY_PATH                 = RESULTS_DIR / \"dataset_summary.csv\"\n",
    "RAW_STATS_PATH               = RESULTS_DIR / \"raw_image_stats.csv\"\n",
    "RAW_RESOLUTION_SUMMARY_PATH  = RESULTS_DIR / \"raw_resolution_summary.csv\"\n",
    "RAW_QUALITY_SUMMARY_PATH     = RESULTS_DIR / \"raw_quality_flags_summary.csv\"\n",
    "RAW_CLASS_COUNTS_PATH        = RESULTS_DIR / \"raw_class_counts_by_source.csv\"\n",
    "DUPLICATES_PATH              = RESULTS_DIR / \"duplicate_files.csv\"\n",
    "DUPLICATE_SUMMARY_PATH       = RESULTS_DIR / \"duplicate_summary.csv\"\n",
    "\n",
    "\n",
    "IMG_SIZE = (224, 224)\n",
    "CLASSES = [\"glioma\", \"meningioma\", \"pituitary\", \"notumor\"]\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Kaggle-aligned split:\n",
    "# - Training -> Train/Val\n",
    "# - Testing  -> Test (held-out)\n",
    "VAL_FRACTION_OF_TRAINING = 0.20   # 80/20 split inside Kaggle Training\n",
    "\n",
    "# Dedup policy:\n",
    "# If an exact duplicate exists in BOTH Training and Testing, keep Testing copy.\n",
    "PREFER_TESTING_ON_CROSS_SPLIT_DUPLICATES = True\n",
    "\n",
    "BACKGROUND_INTENSITY_THRESHOLD = 5\n",
    "CROP_MARGIN = 10\n",
    "\n",
    "# Only process real image files (prevents .DS_Store, Thumbs.db, etc.)\n",
    "ALLOWED_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\"}\n",
    "\n",
    "\n",
    "def _get_resample():\n",
    "    \"\"\"\n",
    "    Pillow version compatibility: Image.Resampling exists in newer versions.\n",
    "    We pick LANCZOS for best downsampling quality.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return Image.Resampling.LANCZOS\n",
    "    except AttributeError:\n",
    "        return Image.LANCZOS\n",
    "\n",
    "\n",
    "RESAMPLE = _get_resample()\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# COLLECTING RAW IMAGE PATHS\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "def collect_images() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Walk through the raw Kaggle folders and build a DataFrame.\n",
    "\n",
    "    For each file record:\n",
    "      - orig_path    : full path on disk\n",
    "      - class        : glioma / meningioma / pituitary / notumor\n",
    "      - source_split : \"training\" or \"testing\" (Kaggle's folder)\n",
    "    \"\"\"\n",
    "    records = []\n",
    "\n",
    "    for split in [\"Training\", \"Testing\"]:\n",
    "        for cls in CLASSES:\n",
    "            folder = RAW_ROOT / split / cls\n",
    "            if not folder.exists():\n",
    "                continue\n",
    "\n",
    "            for p in folder.iterdir():\n",
    "                if not p.is_file():\n",
    "                    continue\n",
    "\n",
    "                # Skip hidden/system files and non-image extensions\n",
    "                if p.name.startswith(\".\"):\n",
    "                    continue\n",
    "                if p.suffix.lower() not in ALLOWED_EXTS:\n",
    "                    continue\n",
    "\n",
    "                records.append(\n",
    "                    {\n",
    "                        \"orig_path\": str(p),\n",
    "                        \"class\": cls,\n",
    "                        \"source_split\": split.lower(),  # \"training\" / \"testing\"\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "    print(f\"Total images found (before deduplication): {len(df)}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def save_raw_class_counts(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Summarise how many images there are per class in Kaggle's\n",
    "    original Training vs Testing folders.\n",
    "\n",
    "    Output: results/raw_class_counts_by_source.csv\n",
    "    \"\"\"\n",
    "    RAW_CLASS_COUNTS_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"No images found; raw class counts not saved.\")\n",
    "        return\n",
    "\n",
    "    counts = (\n",
    "        df.groupby([\"source_split\", \"class\"])\n",
    "        .size()\n",
    "        .reset_index(name=\"count\")\n",
    "        .sort_values([\"source_split\", \"class\"])\n",
    "    )\n",
    "\n",
    "    counts.to_csv(RAW_CLASS_COUNTS_PATH, index=False)\n",
    "    print(f\"Saved raw class counts (by Kaggle split) to {RAW_CLASS_COUNTS_PATH}\")\n",
    "    print(counts)\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# EXACT DUPLICATE REMOVAL (SHA1) - LEAKAGE SAFE\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "def sha1_of_file(path: str, block_size: int = 65536) -> str:\n",
    "    \"\"\"\n",
    "    Compute SHA1 hash of a file.\n",
    "    If two files have the same SHA1, they are byte-for-byte identical.\n",
    "    \"\"\"\n",
    "    h = hashlib.sha1()\n",
    "    with open(path, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(block_size), b\"\"):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def processed_filename_for(orig_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Build a unique, deterministic filename for the processed image\n",
    "    to avoid collisions (same src.name in different folders).\n",
    "\n",
    "    We use SHA1(file_bytes) + original extension.\n",
    "    \"\"\"\n",
    "    p = Path(orig_path)\n",
    "    return f\"{sha1_of_file(str(p))}{p.suffix.lower()}\"\n",
    "\n",
    "\n",
    "def drop_duplicates_leakage_safe(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Remove exact duplicate files based on SHA1, using a leakage-safe policy.\n",
    "\n",
    "    Key rule:\n",
    "      - If a duplicate group spans BOTH Kaggle Training and Kaggle Testing,\n",
    "        we KEEP the Testing copy and DROP the Training copy(s).\n",
    "\n",
    "    Returns:\n",
    "      - dedup_df : DataFrame of kept rows only (unique SHA1)\n",
    "      - dups_df  : DataFrame of all rows that belong to duplicate groups,\n",
    "                  including which file was kept/dropped\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return df.copy(), pd.DataFrame()\n",
    "\n",
    "    print(\"Computing SHA1 hashes for duplicate detection...\")\n",
    "    df = df.copy()\n",
    "    df[\"sha1\"] = df[\"orig_path\"].apply(sha1_of_file)\n",
    "\n",
    "    before = len(df)\n",
    "\n",
    "    # Identify duplicate groups (size >= 2)\n",
    "    dup_mask = df.duplicated(subset=\"sha1\", keep=False)\n",
    "    dups_df = df[dup_mask].copy()\n",
    "\n",
    "    if dups_df.empty:\n",
    "        print(\"No duplicate files detected based on SHA1 hashes.\")\n",
    "        dedup_df = df.reset_index(drop=True)\n",
    "        print(f\"Unique files after deduplication: {len(dedup_df)}\")\n",
    "        return dedup_df, dups_df\n",
    "\n",
    "    keep_indices = []\n",
    "\n",
    "    # For deterministic behaviour, we sort paths inside each group\n",
    "    for sha1, group in dups_df.groupby(\"sha1\"):\n",
    "        g = group.sort_values([\"source_split\", \"orig_path\"]).copy()\n",
    "\n",
    "        has_testing = (g[\"source_split\"] == \"testing\").any()\n",
    "        has_training = (g[\"source_split\"] == \"training\").any()\n",
    "\n",
    "        # Leakage-safe decision:\n",
    "        if PREFER_TESTING_ON_CROSS_SPLIT_DUPLICATES and has_testing and has_training:\n",
    "            # Keep ONE testing copy (deterministic: lexicographically smallest testing path)\n",
    "            keep_row = g[g[\"source_split\"] == \"testing\"].sort_values(\"orig_path\").iloc[0]\n",
    "        else:\n",
    "            # Otherwise keep ONE file deterministically (smallest path)\n",
    "            keep_row = g.sort_values(\"orig_path\").iloc[0]\n",
    "\n",
    "        keep_indices.append(int(keep_row.name))\n",
    "\n",
    "    # Keep all non-duplicates + one representative from each duplicate group\n",
    "    non_dup_df = df[~dup_mask].copy()\n",
    "    kept_from_dups_df = df.loc[keep_indices].copy()\n",
    "\n",
    "    dedup_df = pd.concat([non_dup_df, kept_from_dups_df], axis=0).reset_index(drop=True)\n",
    "\n",
    "    after = len(dedup_df)\n",
    "    print(f\"Unique files after deduplication: {after}\")\n",
    "    print(f\"Removed {before - after} duplicate file entries (if any).\")\n",
    "\n",
    "    # Augment dups_df with keep/drop info for auditing\n",
    "    dups_df = dups_df.copy()\n",
    "    dups_df[\"kept\"] = False\n",
    "\n",
    "    # Mark kept rows inside duplicate groups\n",
    "    kept_set = set(keep_indices)\n",
    "    dups_df.loc[dups_df.index.isin(kept_set), \"kept\"] = True\n",
    "\n",
    "    # Also include which path was kept in the group (helpful for report/audit)\n",
    "    kept_path_by_sha1 = (\n",
    "        dups_df[dups_df[\"kept\"]]\n",
    "        .set_index(\"sha1\")[\"orig_path\"]\n",
    "        .to_dict()\n",
    "    )\n",
    "    dups_df[\"kept_path_for_group\"] = dups_df[\"sha1\"].map(kept_path_by_sha1)\n",
    "\n",
    "    return dedup_df, dups_df\n",
    "\n",
    "\n",
    "def save_duplicate_summary(dups_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Write detailed information about exact duplicates:\n",
    "\n",
    "      - duplicate_files.csv   : full listing of all duplicate-group members,\n",
    "                               including which one was kept\n",
    "      - duplicate_summary.csv : one row per SHA1 group\n",
    "    \"\"\"\n",
    "    DUPLICATES_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if dups_df is None or dups_df.empty:\n",
    "        print(\"No duplicate files to save.\")\n",
    "        return\n",
    "\n",
    "    dups_df.to_csv(DUPLICATES_PATH, index=False)\n",
    "    print(f\"Saved full duplicate listing to {DUPLICATES_PATH}\")\n",
    "\n",
    "    rows = []\n",
    "    for sha1, group in dups_df.groupby(\"sha1\"):\n",
    "        group = group.copy()\n",
    "\n",
    "        kept_rows = group[group[\"kept\"]]\n",
    "        kept_path = kept_rows[\"orig_path\"].iloc[0] if not kept_rows.empty else \"\"\n",
    "\n",
    "        # Helpful flags: duplicates crossing splits, or (rare) crossing classes\n",
    "        crosses_splits = group[\"source_split\"].nunique() > 1\n",
    "        crosses_classes = group[\"class\"].nunique() > 1\n",
    "\n",
    "        rows.append(\n",
    "            {\n",
    "                \"sha1\": sha1,\n",
    "                \"n_files\": len(group),\n",
    "                \"crosses_splits\": bool(crosses_splits),\n",
    "                \"crosses_classes\": bool(crosses_classes),\n",
    "                \"classes\": \";\".join(sorted(group[\"class\"].unique())),\n",
    "                \"source_splits\": \";\".join(sorted(group[\"source_split\"].unique())),\n",
    "                \"kept_path\": kept_path,\n",
    "                \"dropped_paths_example\": \"; \".join(group[~group[\"kept\"]][\"orig_path\"].head(5)),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    summary_df = pd.DataFrame(rows).sort_values(\"n_files\", ascending=False)\n",
    "    summary_df.to_csv(DUPLICATE_SUMMARY_PATH, index=False)\n",
    "    print(f\"Saved duplicate summary to {DUPLICATE_SUMMARY_PATH}\")\n",
    "    print(summary_df.head())\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# RAW IMAGE ANALYSIS (GEOMETRY and INTENSITY)\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "def analyze_raw_images(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Audit each deduplicated raw image before resizing.\n",
    "\n",
    "    For each image record:\n",
    "      - width, height, aspect_ratio\n",
    "      - grayscale intensity stats: mean, std, min, max\n",
    "      - a 'failed' flag if PIL couldn't read the file\n",
    "      - conservative quality flags:\n",
    "          too_dark, too_bright, low_contrast, suspect\n",
    "\n",
    "    This function does NOT drop images; it only flags them.\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    print(\"Analysing raw image geometry and intensity statistics...\")\n",
    "\n",
    "    for row in tqdm(df.to_dict(orient=\"records\"), desc=\"Analysing raw images\"):\n",
    "        path = row[\"orig_path\"]\n",
    "        cls = row[\"class\"]\n",
    "        source_split = row[\"source_split\"]\n",
    "        sha1 = row.get(\"sha1\", None)\n",
    "\n",
    "        width = height = None\n",
    "        aspect_ratio = None\n",
    "        mean_intensity = std_intensity = None\n",
    "        min_intensity = max_intensity = None\n",
    "        failed = False\n",
    "\n",
    "        try:\n",
    "            img = Image.open(path)\n",
    "            img = ImageOps.exif_transpose(img)\n",
    "            img.load()  # force decode to catch corrupt images\n",
    "\n",
    "            width, height = img.size\n",
    "            aspect_ratio = (width / height) if height else np.nan\n",
    "\n",
    "            gray = img.convert(\"L\")\n",
    "            arr = np.array(gray, dtype=np.float32)\n",
    "\n",
    "            mean_intensity = float(arr.mean())\n",
    "            std_intensity = float(arr.std())\n",
    "            min_intensity = float(arr.min())\n",
    "            max_intensity = float(arr.max())\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to analyse {path}: {e}\")\n",
    "            failed = True\n",
    "\n",
    "        records.append(\n",
    "            {\n",
    "                \"orig_path\": path,\n",
    "                \"class\": cls,\n",
    "                \"source_split\": source_split,\n",
    "                \"sha1\": sha1,\n",
    "                \"width\": width,\n",
    "                \"height\": height,\n",
    "                \"aspect_ratio\": aspect_ratio,\n",
    "                \"mean_intensity\": mean_intensity,\n",
    "                \"std_intensity\": std_intensity,\n",
    "                \"min_intensity\": min_intensity,\n",
    "                \"max_intensity\": max_intensity,\n",
    "                \"failed\": failed,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    stats_df = pd.DataFrame(records)\n",
    "    not_failed = ~stats_df[\"failed\"]\n",
    "\n",
    "    stats_df[\"too_dark\"] = (\n",
    "        not_failed\n",
    "        & (stats_df[\"mean_intensity\"] < 15)\n",
    "        & (stats_df[\"max_intensity\"] < 60)\n",
    "    )\n",
    "\n",
    "    stats_df[\"too_bright\"] = (\n",
    "        not_failed\n",
    "        & (stats_df[\"mean_intensity\"] > 240)\n",
    "        & (stats_df[\"min_intensity\"] > 200)\n",
    "    )\n",
    "\n",
    "    stats_df[\"low_contrast\"] = not_failed & (stats_df[\"std_intensity\"] < 5)\n",
    "\n",
    "    stats_df[\"suspect\"] = (\n",
    "        stats_df[\"too_dark\"]\n",
    "        | stats_df[\"too_bright\"]\n",
    "        | stats_df[\"low_contrast\"]\n",
    "        | stats_df[\"failed\"]\n",
    "    )\n",
    "\n",
    "    return stats_df\n",
    "\n",
    "\n",
    "def save_raw_analysis(stats_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Save raw-image analysis to:\n",
    "      - raw_image_stats.csv\n",
    "      - raw_resolution_summary.csv\n",
    "      - raw_quality_flags_summary.csv\n",
    "    \"\"\"\n",
    "    RAW_STATS_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if stats_df.empty:\n",
    "        print(\"No stats to save; stats_df is empty.\")\n",
    "        return\n",
    "\n",
    "    stats_df.to_csv(RAW_STATS_PATH, index=False)\n",
    "    print(f\"Saved raw image stats to {RAW_STATS_PATH}\")\n",
    "\n",
    "    res_summary = (\n",
    "        stats_df.groupby([\"width\", \"height\"])\n",
    "        .size()\n",
    "        .reset_index(name=\"count\")\n",
    "        .sort_values(\"count\", ascending=False)\n",
    "    )\n",
    "    res_summary.to_csv(RAW_RESOLUTION_SUMMARY_PATH, index=False)\n",
    "    print(f\"Saved resolution summary to {RAW_RESOLUTION_SUMMARY_PATH}\")\n",
    "    print(\"Top 5 most common resolutions:\")\n",
    "    print(res_summary.head())\n",
    "\n",
    "    qual_summary = (\n",
    "        stats_df.groupby(\"class\")[[\"too_dark\", \"too_bright\", \"low_contrast\", \"suspect\", \"failed\"]]\n",
    "        .sum()\n",
    "        .reset_index()\n",
    "    )\n",
    "    qual_summary.to_csv(RAW_QUALITY_SUMMARY_PATH, index=False)\n",
    "    print(f\"Saved quality flags summary to {RAW_QUALITY_SUMMARY_PATH}\")\n",
    "    print(qual_summary)\n",
    "\n",
    "    total_suspect = int(stats_df[\"suspect\"].sum())\n",
    "    print(f\"Total suspect images (any flag or failed): {total_suspect}\")\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# TIGHT BRAIN CROPPING - KAGGLE-ALIGNED SPLITS - RESIZED IMAGES\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "def tight_crop_to_brain(img: Image.Image) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Crop away black background around the brain using a simple intensity mask.\n",
    "    If no foreground pixels are found, return the original image.\n",
    "    \"\"\"\n",
    "    gray = img.convert(\"L\")\n",
    "    arr = np.array(gray, dtype=np.uint8)\n",
    "\n",
    "    mask = arr > BACKGROUND_INTENSITY_THRESHOLD\n",
    "    if not mask.any():\n",
    "        return img\n",
    "\n",
    "    ys, xs = np.where(mask)\n",
    "\n",
    "    y_min = max(int(ys.min()) - CROP_MARGIN, 0)\n",
    "    y_max = min(int(ys.max()) + 1 + CROP_MARGIN, arr.shape[0])\n",
    "\n",
    "    x_min = max(int(xs.min()) - CROP_MARGIN, 0)\n",
    "    x_max = min(int(xs.max()) + 1 + CROP_MARGIN, arr.shape[1])\n",
    "\n",
    "    return img.crop((x_min, y_min, x_max, y_max))\n",
    "\n",
    "\n",
    "def make_splits_kaggle_aligned(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Create leakage-safe, Kaggle-aligned splits:\n",
    "\n",
    "      - Train/Val are created ONLY from Kaggle Training images (stratified).\n",
    "      - Test is the Kaggle Testing folder (held-out, no splitting).\n",
    "\n",
    "    This is the correct evaluation setup for this Kaggle dataset:\n",
    "    we do NOT create a new test set from Training because Kaggle already provides one.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        raise ValueError(\"Cannot split an empty dataset.\")\n",
    "\n",
    "    df_train_source = df[df[\"source_split\"] == \"training\"].copy()\n",
    "    df_test_source  = df[df[\"source_split\"] == \"testing\"].copy()\n",
    "\n",
    "    if df_train_source.empty:\n",
    "        raise ValueError(\"No Kaggle Training images found after deduplication.\")\n",
    "    if df_test_source.empty:\n",
    "        print(\"WARNING: No Kaggle Testing images found after deduplication. Test set will be empty.\")\n",
    "\n",
    "    X = df_train_source[\"orig_path\"].values\n",
    "    y = df_train_source[\"class\"].values\n",
    "\n",
    "    # Stratified train/val split inside Kaggle Training only\n",
    "    try:\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X,\n",
    "            y,\n",
    "            test_size=VAL_FRACTION_OF_TRAINING,\n",
    "            stratify=y,\n",
    "            random_state=RANDOM_STATE,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        # Fallback: non-stratified split (should not happen with this dataset, but keeps script robust)\n",
    "        print(f\"WARNING: Stratified split failed ({e}). Falling back to non-stratified split.\")\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X,\n",
    "            y,\n",
    "            test_size=VAL_FRACTION_OF_TRAINING,\n",
    "            random_state=RANDOM_STATE,\n",
    "        )\n",
    "\n",
    "    def to_df(paths, labels, split_name: str) -> pd.DataFrame:\n",
    "        return pd.DataFrame(\n",
    "            {\n",
    "                \"orig_path\": [str(Path(p)) for p in paths],\n",
    "                \"class\": labels,\n",
    "                \"split\": split_name,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    df_train = to_df(X_train, y_train, \"train\")\n",
    "    df_val   = to_df(X_val, y_val, \"val\")\n",
    "\n",
    "    # Kaggle Testing is held-out test\n",
    "    df_test = pd.DataFrame(\n",
    "        {\n",
    "            \"orig_path\": df_test_source[\"orig_path\"].astype(str).values,\n",
    "            \"class\": df_test_source[\"class\"].values,\n",
    "            \"split\": \"test\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(\"Split sizes (Kaggle-aligned, after deduplication):\")\n",
    "    print(f\"  Train (from Kaggle Training): {len(df_train)}\")\n",
    "    print(f\"  Val   (from Kaggle Training): {len(df_val)}\")\n",
    "    print(f\"  Test  (Kaggle Testing):       {len(df_test)}\")\n",
    "\n",
    "    return df_train, df_val, df_test\n",
    "\n",
    "\n",
    "def prepare_processed_dirs():\n",
    "    \"\"\"\n",
    "    Ensure canonical directory structure exists for the cropped variant:\n",
    "\n",
    "        data/processed/tightcrop/train/{class}/\n",
    "        data/processed/tightcrop/val/{class}/\n",
    "        data/processed/tightcrop/test/{class}/\n",
    "    \"\"\"\n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        for cls in CLASSES:\n",
    "            out_dir = PROCESSED_ROOT / split / cls\n",
    "            out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def resize_and_copy(df_split: pd.DataFrame, split_name: str):\n",
    "    \"\"\"\n",
    "    Create the processed dataset:\n",
    "\n",
    "      - read raw image\n",
    "      - exif_transpose (safety)\n",
    "      - tight crop around brain\n",
    "      - resize to IMG_SIZE (224x224) using LANCZOS\n",
    "      - convert to RGB\n",
    "      - save into data/processed/tightcrop/{split}/{class}/\n",
    "\n",
    "    NOTE: Files are saved using SHA1-based filenames to prevent collisions.\n",
    "    \"\"\"\n",
    "    rows = df_split.to_dict(orient=\"records\")\n",
    "\n",
    "    for row in tqdm(rows, desc=f\"Processing {split_name} [{VARIANT}]\"):\n",
    "        src = Path(row[\"orig_path\"])\n",
    "        cls = row[\"class\"]\n",
    "\n",
    "        out_name = processed_filename_for(row[\"orig_path\"])\n",
    "        dst = PROCESSED_ROOT / split_name / cls / out_name\n",
    "\n",
    "        if dst.exists():\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            img = Image.open(src)\n",
    "            img = ImageOps.exif_transpose(img)\n",
    "            img = img.convert(\"RGB\")\n",
    "\n",
    "            img = tight_crop_to_brain(img)\n",
    "            img = img.resize(IMG_SIZE, resample=RESAMPLE)\n",
    "\n",
    "            img.save(dst)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process {src}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "def save_csv_splits(df_train, df_val, df_test):\n",
    "    \"\"\"\n",
    "    Build CSVs that map to the *processed* (cropped) paths.\n",
    "\n",
    "    Each CSV has columns:\n",
    "      - image_path : path to cropped 224x224 RGB image\n",
    "      - class      : tumour class label\n",
    "    \"\"\"\n",
    "    SPLITS_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def map_to_processed(df: pd.DataFrame, split_name: str) -> pd.DataFrame:\n",
    "        processed_paths = []\n",
    "        for _, row in df.iterrows():\n",
    "            cls = row[\"class\"]\n",
    "            out_name = processed_filename_for(row[\"orig_path\"])\n",
    "            processed_paths.append(str(PROCESSED_ROOT / split_name / cls / out_name))\n",
    "        return pd.DataFrame({\"image_path\": processed_paths, \"class\": df[\"class\"].values})\n",
    "\n",
    "    train_csv = map_to_processed(df_train, \"train\")\n",
    "    val_csv   = map_to_processed(df_val, \"val\")\n",
    "    test_csv  = map_to_processed(df_test, \"test\")\n",
    "\n",
    "    train_csv.to_csv(SPLITS_ROOT / \"train.csv\", index=False)\n",
    "    val_csv.to_csv(SPLITS_ROOT / \"val.csv\", index=False)\n",
    "    test_csv.to_csv(SPLITS_ROOT / \"test.csv\", index=False)\n",
    "\n",
    "    print(f\"Saved split CSVs to {SPLITS_ROOT}.\")\n",
    "\n",
    "\n",
    "def save_summary(df_train, df_val, df_test):\n",
    "    \"\"\"\n",
    "    Summarise class counts per split.\n",
    "\n",
    "    Output:\n",
    "      - results/dataset_summary.csv\n",
    "    \"\"\"\n",
    "    SUMMARY_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def counts(df, split_name):\n",
    "        c = df[\"class\"].value_counts().rename(\"count\").reset_index()\n",
    "        c = c.rename(columns={\"index\": \"class\"})\n",
    "        c.insert(0, \"split\", split_name)\n",
    "        return c\n",
    "\n",
    "    summary_df = pd.concat(\n",
    "        [counts(df_train, \"train\"), counts(df_val, \"val\"), counts(df_test, \"test\")],\n",
    "        axis=0,\n",
    "        ignore_index=True,\n",
    "    )\n",
    "\n",
    "    summary_df.to_csv(SUMMARY_PATH, index=False)\n",
    "    print(\"Saved dataset summary to results/dataset_summary.csv\")\n",
    "    print(summary_df)\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# MAIN ORCHESTRATION\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Orchestrate the entire dataset preparation pipeline:\n",
    "\n",
    "      1. Collect raw images from Kaggle folders\n",
    "      2. Save raw class counts per Kaggle split\n",
    "      3. Deduplicate based on SHA1 with leakage-safe policy (prefer keep Testing on cross-split dups)\n",
    "      4. Analyse raw images (geometry + intensity + quality flags)\n",
    "      5. Save analysis CSVs\n",
    "      6. Create Kaggle-aligned splits:\n",
    "           - Train/Val from Kaggle Training (stratified)\n",
    "           - Test from Kaggle Testing (held-out)\n",
    "      7. Write cropped 224x224 RGB images into data/processed/tightcrop/\n",
    "      8. Save train/val/test CSVs and overall summary table\n",
    "    \"\"\"\n",
    "    print(f\"Running dataset preparation for VARIANT = '{VARIANT}' (cropped-only pipeline)\")\n",
    "\n",
    "    # Step 1: collect raw image paths\n",
    "    df_raw = collect_images()\n",
    "\n",
    "    # Step 2: raw class counts per Kaggle split\n",
    "    save_raw_class_counts(df_raw)\n",
    "\n",
    "    # Step 3: exact duplicates (SHA1) with leakage-safe policy\n",
    "    df_dedup, dups_df = drop_duplicates_leakage_safe(df_raw)\n",
    "    save_duplicate_summary(dups_df)\n",
    "\n",
    "    # Step 4: analyse raw images\n",
    "    stats_df = analyze_raw_images(df_dedup)\n",
    "\n",
    "    # Step 5: save analysis artefacts\n",
    "    save_raw_analysis(stats_df)\n",
    "\n",
    "    # Step 6: Kaggle-aligned splits (paper/evaluation correct)\n",
    "    df_train, df_val, df_test = make_splits_kaggle_aligned(df_dedup)\n",
    "\n",
    "    # Step 7: prepare dirs + write processed images\n",
    "    prepare_processed_dirs()\n",
    "    resize_and_copy(df_train, \"train\")\n",
    "    resize_and_copy(df_val, \"val\")\n",
    "    resize_and_copy(df_test, \"test\")\n",
    "\n",
    "    # Step 8: save split CSVs + summary\n",
    "    save_csv_splits(df_train, df_val, df_test)\n",
    "    save_summary(df_train, df_val, df_test)\n",
    "\n",
    "    print(\"Dataset preparation and audit completed (cropped 224x224 images only).\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a5d0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile scripts/train.py\n",
    "\n",
    "# scripts/train.py\n",
    "#\n",
    "# Training + evaluation + plots for HybridResNet50V2â€“RViT.\n",
    "# Early stopping monitors validation macro F1.\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path(__file__).resolve().parents[1]\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    cohen_kappa_score,\n",
    "    matthews_corrcoef,\n",
    ")\n",
    "\n",
    "from models.hybrid_model import HybridResNet50V2_RViT\n",
    "from scripts.data import BrainMRICSV, build_transforms\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device, class_names):\n",
    "    model.eval()\n",
    "    all_y, all_pred, all_prob = [], [], []\n",
    "    total, correct, loss_sum = 0, 0, 0.0\n",
    "    ce = nn.CrossEntropyLoss()\n",
    "\n",
    "    for x, y, _ in loader:\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "        logits, _ = model(x)\n",
    "        loss = ce(logits, y)\n",
    "\n",
    "        prob = torch.softmax(logits, dim=1)\n",
    "        pred = prob.argmax(dim=1)\n",
    "\n",
    "        total += y.size(0)\n",
    "        correct += (pred == y).sum().item()\n",
    "        loss_sum += loss.item() * y.size(0)\n",
    "\n",
    "        all_y.append(y.cpu().numpy())\n",
    "        all_pred.append(pred.cpu().numpy())\n",
    "        all_prob.append(prob.cpu().numpy())\n",
    "\n",
    "    all_y = np.concatenate(all_y)\n",
    "    all_pred = np.concatenate(all_pred)\n",
    "    all_prob = np.concatenate(all_prob)\n",
    "\n",
    "    acc = correct / max(total, 1)\n",
    "    avg_loss = loss_sum / max(total, 1)\n",
    "\n",
    "    rep = classification_report(\n",
    "        all_y, all_pred,\n",
    "        target_names=class_names,\n",
    "        output_dict=True,\n",
    "        zero_division=0\n",
    "    )\n",
    "    macro_f1 = rep[\"macro avg\"][\"f1-score\"]\n",
    "\n",
    "    return {\n",
    "        \"loss\": float(avg_loss),\n",
    "        \"acc\": float(acc),\n",
    "        \"macro_f1\": float(macro_f1),\n",
    "        \"y_true\": all_y,\n",
    "        \"y_pred\": all_pred,\n",
    "        \"y_prob\": all_prob,\n",
    "        \"report\": rep,\n",
    "    }\n",
    "\n",
    "\n",
    "def specificity_macro(cm):\n",
    "    n = cm.shape[0]\n",
    "    specs = []\n",
    "    for c in range(n):\n",
    "        tp = cm[c, c]\n",
    "        fp = cm[:, c].sum() - tp\n",
    "        fn = cm[c, :].sum() - tp\n",
    "        tn = cm.sum() - (tp + fp + fn)\n",
    "        spec = tn / (tn + fp + 1e-12)\n",
    "        specs.append(spec)\n",
    "    return float(np.mean(specs)), [float(s) for s in specs]\n",
    "\n",
    "\n",
    "def plot_training(history, out_path):\n",
    "    epochs = list(range(1, len(history[\"train_loss\"]) + 1))\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, history[\"train_loss\"], label=\"train_loss\")\n",
    "    plt.plot(epochs, history[\"val_loss\"], label=\"val_loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_accuracy(history, out_path):\n",
    "    epochs = list(range(1, len(history[\"train_acc\"]) + 1))\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, history[\"train_acc\"], label=\"train_acc\")\n",
    "    plt.plot(epochs, history[\"val_acc\"], label=\"val_acc\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_confusion(cm, class_names, out_path):\n",
    "    plt.figure()\n",
    "    plt.imshow(cm, interpolation=\"nearest\")\n",
    "    plt.title(\"Confusion Matrix (Test)\")\n",
    "    plt.colorbar()\n",
    "    ticks = np.arange(len(class_names))\n",
    "    plt.xticks(ticks, class_names, rotation=45, ha=\"right\")\n",
    "    plt.yticks(ticks, class_names)\n",
    "\n",
    "    thresh = cm.max() / 2.0\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, str(cm[i, j]),\n",
    "                     ha=\"center\", va=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def build_param_groups(model, cnn_lr, vit_lr, weight_decay):\n",
    "    \"\"\"\n",
    "    Differential LR + no-weight-decay for biases/norm params.\n",
    "    \"\"\"\n",
    "    def is_no_decay(name, p):\n",
    "        if p.ndim == 1:\n",
    "            return True\n",
    "        lname = name.lower()\n",
    "        if lname.endswith(\".bias\"):\n",
    "            return True\n",
    "        if \"bn\" in lname or \"norm\" in lname or \"ln\" in lname:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    cnn_decay, cnn_no = [], []\n",
    "    rest_decay, rest_no = [], []\n",
    "\n",
    "    for name, p in model.named_parameters():\n",
    "        if not p.requires_grad:\n",
    "            continue\n",
    "        target_is_cnn = name.startswith(\"cnn.\")\n",
    "        if is_no_decay(name, p):\n",
    "            (cnn_no if target_is_cnn else rest_no).append(p)\n",
    "        else:\n",
    "            (cnn_decay if target_is_cnn else rest_decay).append(p)\n",
    "\n",
    "    groups = []\n",
    "    if cnn_decay:\n",
    "        groups.append({\"params\": cnn_decay, \"lr\": cnn_lr, \"weight_decay\": weight_decay})\n",
    "    if cnn_no:\n",
    "        groups.append({\"params\": cnn_no, \"lr\": cnn_lr, \"weight_decay\": 0.0})\n",
    "    if rest_decay:\n",
    "        groups.append({\"params\": rest_decay, \"lr\": vit_lr, \"weight_decay\": weight_decay})\n",
    "    if rest_no:\n",
    "        groups.append({\"params\": rest_no, \"lr\": vit_lr, \"weight_decay\": 0.0})\n",
    "\n",
    "    return groups\n",
    "\n",
    "\n",
    "def set_requires_grad(module, flag: bool):\n",
    "    for p in module.parameters():\n",
    "        p.requires_grad = flag\n",
    "\n",
    "\n",
    "def main():\n",
    "    import argparse\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"--csv_dir\", type=str, default=\"data/splits/tightcrop\")\n",
    "    ap.add_argument(\"--out_dir\", type=str, default=\"results/run_hybrid\")\n",
    "\n",
    "    ap.add_argument(\"--epochs\", type=int, default=100)\n",
    "    ap.add_argument(\"--batch_size\", type=int, default=32)\n",
    "\n",
    "    # Differential LR (big impact)\n",
    "    ap.add_argument(\"--cnn_lr\", type=float, default=1e-4)\n",
    "    ap.add_argument(\"--vit_lr\", type=float, default=5e-4)\n",
    "\n",
    "    ap.add_argument(\"--weight_decay\", type=float, default=0.01)\n",
    "    ap.add_argument(\"--patience\", type=int, default=10)\n",
    "    ap.add_argument(\"--seed\", type=int, default=42)\n",
    "    ap.add_argument(\"--num_workers\", type=int, default=2)\n",
    "\n",
    "    # Stabilizers\n",
    "    ap.add_argument(\"--amp\", action=\"store_true\", help=\"Use mixed precision (CUDA)\")\n",
    "    ap.add_argument(\"--grad_clip\", type=float, default=1.0)\n",
    "    ap.add_argument(\"--label_smoothing\", type=float, default=0.05)\n",
    "\n",
    "    # Fine-tuning strategy (Sarada-like usage of pretrained CNN)\n",
    "    ap.add_argument(\"--warmup_epochs\", type=int, default=5, help=\"Freeze CNN for first N epochs\")\n",
    "    ap.add_argument(\"--freeze_cnn_bn\", action=\"store_true\", help=\"Keep CNN BatchNorm frozen (recommended)\")\n",
    "\n",
    "    # Model config\n",
    "    ap.add_argument(\"--cnn_name\", type=str, default=\"resnetv2_50x1_bitm\")\n",
    "    ap.add_argument(\"--patch_size\", type=int, default=16)\n",
    "    ap.add_argument(\"--embed_dim\", type=int, default=142)\n",
    "    ap.add_argument(\"--depth\", type=int, default=10)\n",
    "    ap.add_argument(\"--heads\", type=int, default=10)\n",
    "    ap.add_argument(\"--mlp_dim\", type=int, default=480)\n",
    "    ap.add_argument(\"--attn_dropout\", type=float, default=0.1)\n",
    "    ap.add_argument(\"--vit_dropout\", type=float, default=0.1)\n",
    "    ap.add_argument(\"--fusion_dim\", type=int, default=256)\n",
    "    ap.add_argument(\"--fusion_dropout\", type=float, default=0.5)\n",
    "\n",
    "    args = ap.parse_args()\n",
    "\n",
    "    project_root = str(Path(__file__).resolve().parents[1])\n",
    "    out_dir = Path(args.out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    set_seed(args.seed)\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    use_amp = bool(args.amp and device == \"cuda\")\n",
    "\n",
    "    class_names = [\"glioma\", \"meningioma\", \"pituitary\", \"notumor\"]\n",
    "\n",
    "    # Keep your normalization unless you have dataset-specific mean/std ready.\n",
    "    mean = (0.5, 0.5, 0.5)\n",
    "    std = (0.5, 0.5, 0.5)\n",
    "\n",
    "    train_tf = build_transforms(train=True, mean=mean, std=std)\n",
    "    eval_tf = build_transforms(train=False, mean=mean, std=std)\n",
    "\n",
    "    train_ds = BrainMRICSV(os.path.join(args.csv_dir, \"train.csv\"), class_names, train_tf, project_root)\n",
    "    val_ds = BrainMRICSV(os.path.join(args.csv_dir, \"val.csv\"), class_names, eval_tf, project_root)\n",
    "    test_ds = BrainMRICSV(os.path.join(args.csv_dir, \"test.csv\"), class_names, eval_tf, project_root)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=args.batch_size, shuffle=True,\n",
    "                              num_workers=args.num_workers, pin_memory=True, drop_last=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=args.batch_size, shuffle=False,\n",
    "                            num_workers=args.num_workers, pin_memory=True)\n",
    "    test_loader = DataLoader(test_ds, batch_size=args.batch_size, shuffle=False,\n",
    "                             num_workers=args.num_workers, pin_memory=True)\n",
    "\n",
    "    model = HybridResNet50V2_RViT(\n",
    "        num_classes=4,\n",
    "        patch_size=args.patch_size,\n",
    "        embed_dim=args.embed_dim,\n",
    "        depth=args.depth,\n",
    "        heads=args.heads,\n",
    "        mlp_dim=args.mlp_dim,\n",
    "        attn_dropout=args.attn_dropout,\n",
    "        vit_dropout=args.vit_dropout,\n",
    "        fusion_dim=args.fusion_dim,\n",
    "        fusion_dropout=args.fusion_dropout,\n",
    "        rotations=(0, 1, 2, 3),\n",
    "        cnn_name=args.cnn_name,\n",
    "        cnn_pretrained=True,\n",
    "    ).to(device)\n",
    "\n",
    "    # Loss with label smoothing (small but helps multi-class generalisation)\n",
    "    ce = nn.CrossEntropyLoss(label_smoothing=float(args.label_smoothing))\n",
    "\n",
    "    # Warmup: freeze CNN first\n",
    "    if args.warmup_epochs > 0:\n",
    "        set_requires_grad(model.cnn, False)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        build_param_groups(model, cnn_lr=args.cnn_lr, vit_lr=args.vit_lr, weight_decay=args.weight_decay)\n",
    "    )\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs, eta_min=1e-6)\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "\n",
    "    history = {\n",
    "        \"train_loss\": [],\n",
    "        \"val_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"val_acc\": [],\n",
    "        \"val_macro_f1\": [],\n",
    "        \"epoch_time_sec\": [],\n",
    "    }\n",
    "\n",
    "    best_f1 = -1.0\n",
    "    best_epoch = -1\n",
    "    bad_epochs = 0\n",
    "\n",
    "    start_train = time.time()\n",
    "\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Unfreeze after warmup\n",
    "        if epoch == args.warmup_epochs + 1:\n",
    "            set_requires_grad(model.cnn, True)\n",
    "            # Rebuild optimizer so CNN params are included with cnn_lr\n",
    "            optimizer = torch.optim.AdamW(\n",
    "                build_param_groups(model, cnn_lr=args.cnn_lr, vit_lr=args.vit_lr, weight_decay=args.weight_decay)\n",
    "            )\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=(args.epochs - epoch + 1), eta_min=1e-6)\n",
    "            scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        # Freeze CNN BN stats if requested (often stabilizes transfer learning)\n",
    "        if args.freeze_cnn_bn:\n",
    "            model.freeze_cnn_bn()\n",
    "\n",
    "        total, correct, loss_sum = 0, 0, 0.0\n",
    "\n",
    "        for x, y, _ in train_loader:\n",
    "            x = x.to(device, non_blocking=True)\n",
    "            y = y.to(device, non_blocking=True)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "                logits, _ = model(x)\n",
    "                loss = ce(logits, y)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            # Grad clip (very important for the val_loss spikes you saw)\n",
    "            if args.grad_clip and args.grad_clip > 0:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=float(args.grad_clip))\n",
    "\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            prob = torch.softmax(logits.detach(), dim=1)\n",
    "            pred = prob.argmax(dim=1)\n",
    "\n",
    "            total += y.size(0)\n",
    "            correct += (pred == y).sum().item()\n",
    "            loss_sum += float(loss.item()) * y.size(0)\n",
    "\n",
    "        train_loss = loss_sum / max(total, 1)\n",
    "        train_acc = correct / max(total, 1)\n",
    "\n",
    "        val_stats = evaluate(model, val_loader, device, class_names)\n",
    "        val_loss = val_stats[\"loss\"]\n",
    "        val_acc = val_stats[\"acc\"]\n",
    "        val_f1 = val_stats[\"macro_f1\"]\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        history[\"train_loss\"].append(float(train_loss))\n",
    "        history[\"val_loss\"].append(float(val_loss))\n",
    "        history[\"train_acc\"].append(float(train_acc))\n",
    "        history[\"val_acc\"].append(float(val_acc))\n",
    "        history[\"val_macro_f1\"].append(float(val_f1))\n",
    "        history[\"epoch_time_sec\"].append(float(time.time() - t0))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch:03d} | \"\n",
    "            f\"train_loss={train_loss:.4f} train_acc={train_acc:.4f} | \"\n",
    "            f\"val_loss={val_loss:.4f} val_acc={val_acc:.4f} val_macroF1={val_f1:.4f}\"\n",
    "        )\n",
    "\n",
    "        # Early stopping on macro F1\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            best_epoch = epoch\n",
    "            bad_epochs = 0\n",
    "\n",
    "            ckpt = {\n",
    "                \"model_state\": model.state_dict(),\n",
    "                \"class_names\": class_names,\n",
    "                \"mean\": mean,\n",
    "                \"std\": std,\n",
    "                \"train_args\": vars(args),\n",
    "                \"model_cfg\": {\n",
    "                    \"num_classes\": 4,\n",
    "                    \"patch_size\": args.patch_size,\n",
    "                    \"embed_dim\": args.embed_dim,\n",
    "                    \"depth\": args.depth,\n",
    "                    \"heads\": args.heads,\n",
    "                    \"mlp_dim\": args.mlp_dim,\n",
    "                    \"attn_dropout\": args.attn_dropout,\n",
    "                    \"vit_dropout\": args.vit_dropout,\n",
    "                    \"fusion_dim\": args.fusion_dim,\n",
    "                    \"fusion_dropout\": args.fusion_dropout,\n",
    "                    \"rotations\": (0, 1, 2, 3),\n",
    "                    \"cnn_name\": args.cnn_name,\n",
    "                },\n",
    "            }\n",
    "            torch.save(ckpt, out_dir / \"best_model.pt\")\n",
    "        else:\n",
    "            bad_epochs += 1\n",
    "            if bad_epochs >= args.patience:\n",
    "                print(f\"Early stopping at epoch {epoch} (best epoch: {best_epoch}, best macroF1: {best_f1:.4f})\")\n",
    "                break\n",
    "\n",
    "    total_train_time = time.time() - start_train\n",
    "    print(f\"Total training time (sec): {total_train_time:.1f}\")\n",
    "\n",
    "    # Save history\n",
    "    import pandas as pd\n",
    "    pd.DataFrame(history).to_csv(out_dir / \"history.csv\", index=False)\n",
    "\n",
    "    # Plots\n",
    "    plot_training(history, out_dir / \"loss_curves.png\")\n",
    "    plot_accuracy(history, out_dir / \"acc_curves.png\")\n",
    "\n",
    "    # Test eval using best model\n",
    "    best = torch.load(out_dir / \"best_model.pt\", map_location=device)\n",
    "    model.load_state_dict(best[\"model_state\"])\n",
    "\n",
    "    test_stats = evaluate(model, test_loader, device, class_names)\n",
    "    y_true = test_stats[\"y_true\"]\n",
    "    y_pred = test_stats[\"y_pred\"]\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(len(class_names))))\n",
    "    plot_confusion(cm, class_names, out_dir / \"confusion_matrix.png\")\n",
    "\n",
    "    rep = test_stats[\"report\"]\n",
    "    kappa = cohen_kappa_score(y_true, y_pred)\n",
    "    mcc = matthews_corrcoef(y_true, y_pred)\n",
    "    spec_macro, spec_per_class = specificity_macro(cm)\n",
    "\n",
    "    metrics = {\n",
    "        \"test_loss\": test_stats[\"loss\"],\n",
    "        \"test_acc\": test_stats[\"acc\"],\n",
    "        \"per_class\": {c: rep[c] for c in class_names},\n",
    "        \"macro_avg\": rep[\"macro avg\"],\n",
    "        \"weighted_avg\": rep[\"weighted avg\"],\n",
    "        \"specificity_macro\": spec_macro,\n",
    "        \"specificity_per_class\": {class_names[i]: spec_per_class[i] for i in range(len(class_names))},\n",
    "        \"cohens_kappa\": float(kappa),\n",
    "        \"mcc_multiclass\": float(mcc),\n",
    "        \"confusion_matrix\": cm.tolist(),\n",
    "        \"num_parameters\": int(sum(p.numel() for p in model.parameters())),\n",
    "        \"epoch_time_sec_mean\": float(np.mean(history[\"epoch_time_sec\"])) if len(history[\"epoch_time_sec\"]) else None,\n",
    "        \"total_training_time_sec\": float(total_train_time),\n",
    "        \"best_epoch\": int(best_epoch),\n",
    "        \"best_val_macro_f1\": float(best_f1),\n",
    "    }\n",
    "\n",
    "    with open(out_dir / \"metrics.json\", \"w\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "\n",
    "    print(\"Saved outputs to:\", str(out_dir))\n",
    "    print(\"Best epoch:\", best_epoch, \"Best val macroF1:\", best_f1)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537a3583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scripts/predict_xai_with_pfd_gste.py\n",
    "#\n",
    "# STRICT \"WITH PFD+GSTE\" version:\n",
    "# - Forces use_pfd_gste=True when rebuilding model.\n",
    "# - Grad-CAM++ from PFD output feature map (feat_path).\n",
    "# - Attention rollout uses returned attention list and uses gste_side for reshape.\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "ROOT = Path(__file__).resolve().parents[1]\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(ROOT))\n",
    "\n",
    "from models.hybrid_model import HybridResNet50V2_RViT\n",
    "\n",
    "\n",
    "def preprocess_pil(img, mean, std):\n",
    "    img = img.convert(\"RGB\")\n",
    "    x = torch.from_numpy(np.array(img)).float() / 255.0\n",
    "    x = x.permute(2, 0, 1).contiguous()\n",
    "    mean_t = torch.tensor(mean, dtype=x.dtype).view(3, 1, 1)\n",
    "    std_t = torch.tensor(std, dtype=x.dtype).view(3, 1, 1)\n",
    "    x = (x - mean_t) / std_t\n",
    "    return x.unsqueeze(0)\n",
    "\n",
    "\n",
    "def normalize_map(h):\n",
    "    h = h - h.min()\n",
    "    h = h / (h.max() + 1e-8)\n",
    "    return h\n",
    "\n",
    "\n",
    "def gradcam_pp_from_activations(A, grads):\n",
    "    if A is None:\n",
    "        raise RuntimeError(\"Activation A is None (hook failed).\")\n",
    "    if grads is None:\n",
    "        raise RuntimeError(\"Gradients missing. Ensure enable_grad + backward ran.\")\n",
    "\n",
    "    grad_1 = grads\n",
    "    grad_2 = grad_1 ** 2\n",
    "    grad_3 = grad_2 * grad_1\n",
    "\n",
    "    spatial_sum = (A * grad_3).sum(dim=(2, 3), keepdim=True)\n",
    "    denom = 2.0 * grad_2 + spatial_sum + 1e-8\n",
    "    alpha = grad_2 / denom\n",
    "\n",
    "    w = (alpha * F.relu(grad_1)).sum(dim=(2, 3), keepdim=True)\n",
    "    cam = (w * A).sum(dim=1, keepdim=True)\n",
    "    cam = F.relu(cam).squeeze(0).squeeze(0)\n",
    "    return normalize_map(cam)\n",
    "\n",
    "\n",
    "def attention_rollout(attn_list, side=None, eps=1e-6):\n",
    "    if not attn_list:\n",
    "        raise RuntimeError(\"No attention matrices captured. Ensure return_xai=True.\")\n",
    "\n",
    "    mats = []\n",
    "    for attn in attn_list:\n",
    "        # attn: (B, H, N, N)\n",
    "        a = attn.mean(dim=1)  # (B, N, N)\n",
    "        n = a.shape[-1]\n",
    "        a = a + torch.eye(n, device=a.device).unsqueeze(0)\n",
    "        a = a / (a.sum(dim=-1, keepdim=True) + eps)\n",
    "        mats.append(a)\n",
    "\n",
    "    R = mats[0]\n",
    "    for i in range(1, len(mats)):\n",
    "        R = R @ mats[i]\n",
    "\n",
    "    # global token importance (no CLS): average over query tokens -> importance of each key token\n",
    "    r = R.mean(dim=1).squeeze(0)  # (N,)\n",
    "    r = normalize_map(r)\n",
    "\n",
    "    N = int(r.shape[0])\n",
    "\n",
    "    if side is not None:\n",
    "        side = int(side)\n",
    "        if side * side != N:\n",
    "            raise RuntimeError(f\"gste_side mismatch: side={side} but N={N}\")\n",
    "        return r.reshape(side, side)\n",
    "\n",
    "    # fallback: infer square\n",
    "    ht = int(round(float(np.sqrt(N))))\n",
    "    ht = max(ht, 1)\n",
    "    wt = max(N // ht, 1)\n",
    "    if ht * wt != N:\n",
    "        ht, wt = 1, N\n",
    "    return r.reshape(ht, wt)\n",
    "\n",
    "\n",
    "def overlay_and_save(img_rgb, heat, out_path, title):\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(img_rgb)\n",
    "    plt.imshow(heat, alpha=0.45)\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(str(out_path), dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def main():\n",
    "    import argparse\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"--checkpoint\", type=str, required=True)\n",
    "    ap.add_argument(\"--image\", type=str, required=True)\n",
    "    ap.add_argument(\"--out_dir\", type=str, default=\"results/xai_with_pfd_gste\")\n",
    "    ap.add_argument(\"--mc_samples\", type=int, default=20)\n",
    "    ap.add_argument(\"--target_class\", type=int, default=-1)\n",
    "    args = ap.parse_args()\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    out_dir = Path(args.out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    ckpt = torch.load(args.checkpoint, map_location=device)\n",
    "    class_names = ckpt.get(\"class_names\", [\"glioma\", \"meningioma\", \"pituitary\", \"notumor\"])\n",
    "    mean = ckpt.get(\"mean\", [0.5, 0.5, 0.5])\n",
    "    std = ckpt.get(\"std\", [0.5, 0.5, 0.5])\n",
    "    cfg = ckpt.get(\"model_cfg\", {})\n",
    "\n",
    "    # FORCE \"with PFD+GSTE\" for this script\n",
    "    model = HybridResNet50V2_RViT(\n",
    "        num_classes=cfg.get(\"num_classes\", len(class_names)),\n",
    "        patch_size=cfg.get(\"patch_size\", 16),\n",
    "        embed_dim=cfg.get(\"embed_dim\", 142),\n",
    "        depth=cfg.get(\"depth\", 10),\n",
    "        heads=cfg.get(\"heads\", 10),\n",
    "        mlp_dim=cfg.get(\"mlp_dim\", 480),\n",
    "        attn_dropout=cfg.get(\"attn_dropout\", 0.1),\n",
    "        vit_dropout=cfg.get(\"vit_dropout\", 0.1),\n",
    "        fusion_dim=cfg.get(\"fusion_dim\", 256),\n",
    "        fusion_dropout=cfg.get(\"fusion_dropout\", 0.5),\n",
    "        rotations=tuple(cfg.get(\"rotations\", (0, 1, 2, 3))),\n",
    "        cnn_name=cfg.get(\"cnn_name\", \"resnetv2_50x1_bitm\"),\n",
    "        cnn_pretrained=False,\n",
    "        use_pfd_gste=True,\n",
    "        # if you ever store these in ckpt later, you can read them too:\n",
    "        # gste_min_side=cfg.get(\"gste_min_side\", 7),\n",
    "        # gste_std_flat=cfg.get(\"gste_std_flat\", 0.05),\n",
    "        # gste_std_full=cfg.get(\"gste_std_full\", 0.30),\n",
    "        # gste_max_shrink=cfg.get(\"gste_max_shrink\", 0.50),\n",
    "    ).to(device)\n",
    "\n",
    "    state = ckpt.get(\"model_state\") or ckpt.get(\"state_dict\")\n",
    "    if state is None:\n",
    "        raise KeyError(\"Checkpoint missing weights ('model_state' or 'state_dict').\")\n",
    "    model.load_state_dict(state)\n",
    "    model.eval()\n",
    "\n",
    "    img = Image.open(args.image).convert(\"RGB\").resize((224, 224))\n",
    "    x = preprocess_pil(img, mean, std).to(device)\n",
    "\n",
    "    hook_cache = {}\n",
    "\n",
    "    def _pfd_hook(module, inputs, output):\n",
    "        if not isinstance(output, (tuple, list)) or len(output) < 1:\n",
    "            raise RuntimeError(\"PFD hook expected tuple/list output (feat_path, mask).\")\n",
    "        feat_path = output[0]\n",
    "        feat_path.retain_grad()\n",
    "        hook_cache[\"cnn_feat\"] = feat_path\n",
    "\n",
    "    h = model.pfd.register_forward_hook(_pfd_hook)\n",
    "    try:\n",
    "        with torch.enable_grad():\n",
    "            logits, xai = model(x, return_xai=True)\n",
    "\n",
    "        if not isinstance(xai, dict):\n",
    "            raise RuntimeError(\"Expected xai dict from model when return_xai=True.\")\n",
    "        if xai.get(\"mask\", None) is None:\n",
    "            raise RuntimeError(\"mask is None => PFD/GSTE not active in forward. Wrong checkpoint or wrong model mode.\")\n",
    "\n",
    "        prob = torch.softmax(logits, dim=1).squeeze(0)\n",
    "        pred_idx = int(prob.argmax().item())\n",
    "        conf = float(prob[pred_idx].item())\n",
    "        target = pred_idx if args.target_class < 0 else int(args.target_class)\n",
    "\n",
    "        model.zero_grad(set_to_none=True)\n",
    "        logits[0, target].backward()\n",
    "\n",
    "        A = hook_cache.get(\"cnn_feat\", None)\n",
    "        if A is None or A.grad is None:\n",
    "            raise RuntimeError(\"Failed to capture PFD feature map gradients for Grad-CAM++.\")\n",
    "\n",
    "    finally:\n",
    "        h.remove()\n",
    "\n",
    "    cam_small = gradcam_pp_from_activations(A, A.grad)\n",
    "    cam = F.interpolate(cam_small.unsqueeze(0).unsqueeze(0), size=(224, 224),\n",
    "                        mode=\"bilinear\", align_corners=False).squeeze()\n",
    "    cam = cam.detach().cpu().numpy()\n",
    "\n",
    "    attn_list = xai.get(\"attn\") or xai.get(\"attn_list\")\n",
    "    if attn_list is None:\n",
    "        raise KeyError(\"XAI dict missing attention list: expected 'attn' or 'attn_list'.\")\n",
    "\n",
    "    gste_side = xai.get(\"gste_side\", None)\n",
    "    attn_tok = attention_rollout(attn_list, side=gste_side)\n",
    "    attn_img = F.interpolate(attn_tok.unsqueeze(0).unsqueeze(0), size=(224, 224),\n",
    "                             mode=\"bilinear\", align_corners=False).squeeze()\n",
    "    attn_img = attn_img.detach().cpu().numpy()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        mu, var = model.mc_dropout_predict(x, mc_samples=args.mc_samples)\n",
    "        mu = mu.squeeze(0).cpu().numpy()\n",
    "        var = var.squeeze(0).cpu().numpy()\n",
    "    mu_pred = int(np.argmax(mu))\n",
    "    mu_conf = float(mu[mu_pred])\n",
    "    mu_var = float(var[mu_pred])\n",
    "\n",
    "    img_rgb = np.array(img)\n",
    "    overlay_and_save(img_rgb, cam, out_dir / \"xai_gradcampp.png\",\n",
    "                     f\"Grad-CAM++ (PFD target={class_names[target]})\")\n",
    "    overlay_and_save(img_rgb, attn_img, out_dir / \"xai_attn_rollout.png\",\n",
    "                     \"Attention Rollout\")\n",
    "\n",
    "    print(\"Prediction (single pass):\")\n",
    "    print(f\"  class = {class_names[pred_idx]} (idx={pred_idx})\")\n",
    "    print(f\"  confidence = {conf:.4f}\")\n",
    "    print(\"\\nMC Dropout:\")\n",
    "    print(f\"  mean-pred class = {class_names[mu_pred]} (idx={mu_pred})\")\n",
    "    print(f\"  mean confidence = {mu_conf:.4f}\")\n",
    "    print(f\"  predictive variance = {mu_var:.6f}\")\n",
    "    print(\"\\nSaved XAI:\")\n",
    "    print(out_dir / \"xai_gradcampp.png\")\n",
    "    print(out_dir / \"xai_attn_rollout.png\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c201d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile models/hybrid_model.py\n",
    "# models/hybrid_model.py  # file target (Jupyter magic writes this file)\n",
    "#\n",
    "# Hybrid ResNet50V2â€“RViT (PFDB-GSTEB): paper-faithful where it matters + my extensions.  # high-level summary\n",
    "# - Sarada baseline: pretrained ResNet50V2 backbone, then add head regularisation (BN/Dropout) without breaking backbone.  # Sarada-style framing (Sarada et al., 2024)\n",
    "# - Krishnan RViT: rotate IMAGE {0,90,180,270} -> patchify P=16 -> (pos + rot) embeddings -> average -> encoder.  # RViT pipeline (Krishnan et al., 2024)\n",
    "# - Encoder block: MHSA + depth-wise conv + MLP; token global average pooling (no CLS).  # Krishnan block style\n",
    "# - PFD-B (my extension): 1Ã—1 conv + sigmoid mask on CNN feature map; gating affects CNN pooled descriptor and guides transformer tokens.  # dual-branch gating idea\n",
    "# - GSTE-B (my extension): mask-guided dynamic token evolution (optional grid shrink) + interpolated positional embeddings.  # dynamic token grid\n",
    "# - XAI: attention list for rollout; image-scale mask for overlay; CNN feature map saved for Grad-CAM++ hooks.  # interpretability outputs\n",
    "#\n",
    "# Libraries: PyTorch is my core DL stack (Paszke et al., 2019).  # torch citation pointer\n",
    "\n",
    "import math  # sqrt/ceil utilities (Python stdlib)\n",
    "import torch  # tensors + autograd (Paszke et al., 2019)\n",
    "import torch.nn as nn  # layers (Paszke et al., 2019)\n",
    "import torch.nn.functional as F  # functional ops (Paszke et al., 2019)\n",
    "\n",
    "# -------------------------  # section divider\n",
    "# CNN: pretrained ResNet50V2 (timm)  # backbone source\n",
    "# -------------------------  # section divider\n",
    "class ResNet50V2TimmBackbone(nn.Module):  # wraps timm backbone\n",
    "    \"\"\"  # docstring start\n",
    "    Uses timm pretrained ResNetV2 feature extractor. Returns last feature map (B, C, 7, 7) for 224x224 input.  # output description\n",
    "    \"\"\"  # docstring end\n",
    "    def __init__(self, model_name=\"resnetv2_50x1_bitm\", pretrained=True):  # config\n",
    "        super().__init__()  # init base\n",
    "        try:  # guarded import\n",
    "            import timm  # timm model zoo; BiT-style weights common (Kolesnikov et al., 2020)\n",
    "        except Exception as e:  # missing timm\n",
    "            raise ImportError(  # raise helpful error\n",
    "                \"timm is required for pretrained ResNet50V2. \"\n",
    "                \"Install timm or use an environment (e.g., Kaggle) that includes it.\"\n",
    "            ) from e  # keep traceback\n",
    "        self.model_name = model_name  # store name\n",
    "        self.backbone = timm.create_model(  # create model\n",
    "            model_name,  # id\n",
    "            pretrained=pretrained,  # weights\n",
    "            features_only=True,  # feature maps\n",
    "            out_indices=(4,),  # last stage\n",
    "        )  # end create\n",
    "        try:  # safe channel query\n",
    "            self.out_ch = int(self.backbone.feature_info.channels()[-1])  # channel count for last stage\n",
    "        except Exception:  # fallback\n",
    "            self.out_ch = 2048  # common ResNet last stage channels\n",
    "    def forward(self, x):  # forward\n",
    "        feats = self.backbone(x)  # list of features\n",
    "        return feats[-1]  # last stage feature map\n",
    "\n",
    "# -------------------------  # section divider\n",
    "# PFD: pathology mask gating (Krsna extension)  # learned spatial gate\n",
    "# -------------------------  # section divider\n",
    "class PFD(nn.Module):  # pathology-focused disentanglement block\n",
    "    \"\"\"  # docstring start\n",
    "    PFD (Krsna): learned pathology gate on CNN feature maps. M = sigmoid(conv1x1(F)), F_path = M * F  # gating equation\n",
    "    \"\"\"  # docstring end\n",
    "    def __init__(self, in_ch):  # in channels\n",
    "        super().__init__()  # init base\n",
    "        self.mask_conv = nn.Conv2d(in_ch, 1, kernel_size=1, stride=1, padding=0, bias=True)  # 1Ã—1 mask predictor\n",
    "    def forward(self, feat):  # feat: (B,C,7,7)\n",
    "        mask = torch.sigmoid(self.mask_conv(feat))  # (B,1,7,7) gate in [0,1]\n",
    "        gated = feat * mask  # apply gate (broadcast)\n",
    "        return gated, mask  # return gated feat + mask\n",
    "\n",
    "# -------------------------  # section divider\n",
    "# Krishnan RViT: patchify IMAGE (P=16) -> tokens (B,N,D)  # raw-image patchify path\n",
    "# -------------------------  # section divider\n",
    "class ImagePatchEmbed(nn.Module):  # patch embedding module\n",
    "    \"\"\"  # docstring start\n",
    "    Patch embedding as linear projection of flattened patches. Conv2d(kernel=P, stride=P) is equivalent to linear patch projection in ViT.  # ViT equivalence\n",
    "    \"\"\"  # docstring end\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=142):  # config\n",
    "        super().__init__()  # init base\n",
    "        self.img_size = img_size  # store image size\n",
    "        self.patch_size = patch_size  # store patch size\n",
    "        self.grid_h = img_size // patch_size  # grid height (14 for 224/16)\n",
    "        self.grid_w = img_size // patch_size  # grid width (14 for 224/16)\n",
    "        self.num_patches = self.grid_h * self.grid_w  # number of tokens (196)\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size, bias=True)  # conv patch projection\n",
    "    def forward(self, x):  # x: (B,3,H,W)\n",
    "        x = self.proj(x)  # (B,D,gh,gw)\n",
    "        B, D, H, W = x.shape  # unpack projected map\n",
    "        tokens = x.flatten(2).transpose(1, 2)  # (B,N,D)\n",
    "        return tokens, H, W  # return tokens + grid\n",
    "\n",
    "class PositionalAndRotationEmbedding(nn.Module):  # adds pos+rot embeddings\n",
    "    \"\"\"  # docstring start\n",
    "    Learnable positional embeddings + rotation embeddings (Krishnan-style). Default base grid for 224, P=16 is 14x14.  # expected grid\n",
    "    \"\"\"  # docstring end\n",
    "    def __init__(self, base_h=14, base_w=14, embed_dim=142, n_rot=4):  # config\n",
    "        super().__init__()  # init base\n",
    "        self.base_h = base_h  # base height\n",
    "        self.base_w = base_w  # base width\n",
    "        self.embed_dim = embed_dim  # token dim\n",
    "        self.n_rot = int(n_rot)  # number of rotation embeddings\n",
    "        self.pos = nn.Parameter(torch.zeros(1, base_h * base_w, embed_dim))  # learnable pos table\n",
    "        nn.init.trunc_normal_(self.pos, std=0.02)  # init\n",
    "        self.rot = nn.Parameter(torch.zeros(self.n_rot, embed_dim))  # learnable rot table\n",
    "        nn.init.trunc_normal_(self.rot, std=0.02)  # init\n",
    "    def forward(self, tokens, ht, wt, rot_idx):  # tokens (B,N,D), grid ht/wt, rotation id\n",
    "        if ht == self.base_h and wt == self.base_w:  # if grid matches base\n",
    "            pos = self.pos  # use directly\n",
    "        else:  # otherwise interpolate pos\n",
    "            pos = self.pos.reshape(1, self.base_h, self.base_w, self.embed_dim).permute(0, 3, 1, 2)  # (1,D,H,W)\n",
    "            pos = F.interpolate(pos, size=(ht, wt), mode=\"bilinear\", align_corners=False)  # resize\n",
    "            pos = pos.permute(0, 2, 3, 1).reshape(1, ht * wt, self.embed_dim)  # back to (1,N,D)\n",
    "        r = int(rot_idx) % self.n_rot  # keep rotation index in range\n",
    "        tokens = tokens + pos + self.rot[r].view(1, 1, -1)  # add pos + rot\n",
    "        return tokens  # return embedded tokens\n",
    "\n",
    "# -------------------------  # section divider\n",
    "# Flexible MHSA (supports dim not divisible by heads)  # attention core\n",
    "# -------------------------  # section divider\n",
    "class FlexibleMHSA(nn.Module):  # attention module\n",
    "    \"\"\"  # docstring start\n",
    "    Flexible MHSA for dim not divisible by heads. Uses inner_dim = heads * ceil(dim/heads) so we do NOT drop dimensions.  # PFDB uses ceil\n",
    "    Attention runs in inner_dim, then projected back to dim.  # projection back\n",
    "    \"\"\"  # docstring end\n",
    "    def __init__(self, dim, num_heads, attn_dropout=0.1, proj_dropout=0.1):  # config\n",
    "        super().__init__()  # init base\n",
    "        self.dim = dim  # store dim\n",
    "        self.num_heads = num_heads  # store heads\n",
    "        head_dim = int(math.ceil(dim / num_heads))  # ceil division for head_dim\n",
    "        inner_dim = num_heads * head_dim  # internal dim\n",
    "        self.inner_dim = inner_dim  # store\n",
    "        self.head_dim = head_dim  # store\n",
    "        self.scale = head_dim ** -0.5  # scale factor\n",
    "        self.qkv = nn.Linear(dim, inner_dim * 3, bias=True)  # qkv projection\n",
    "        self.attn_drop = nn.Dropout(attn_dropout)  # attn dropout\n",
    "        self.proj = nn.Linear(inner_dim, dim, bias=True)  # output projection\n",
    "        self.proj_drop = nn.Dropout(proj_dropout)  # proj dropout\n",
    "    def forward(self, x, return_attn=False):  # x: (B,N,D)\n",
    "        B, N, D = x.shape  # unpack\n",
    "        qkv = self.qkv(x)  # (B,N,3*inner_dim)\n",
    "        qkv = qkv.reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)  # (3,B,H,N,hd)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # split q,k,v\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale  # logits\n",
    "        attn = attn.softmax(dim=-1)  # normalize\n",
    "        attn = self.attn_drop(attn)  # dropout\n",
    "        out = attn @ v  # apply attention\n",
    "        out = out.transpose(1, 2).reshape(B, N, self.inner_dim)  # merge heads\n",
    "        out = self.proj(out)  # project back to dim\n",
    "        out = self.proj_drop(out)  # dropout\n",
    "        if return_attn:  # if requested\n",
    "            return out, attn  # return attention too\n",
    "        return out, None  # otherwise no attn\n",
    "\n",
    "class RViTBlock(nn.Module):  # RViT block (MHSA + DWConv + MLP)\n",
    "    \"\"\"  # docstring start\n",
    "    MHSA -> DWConv -> MLP, each with pre-LN + residual.  # structure\n",
    "    \"\"\"  # docstring end\n",
    "    def __init__(self, dim, heads, mlp_dim, attn_dropout=0.1, dropout=0.1, ht=14, wt=14):  # config\n",
    "        super().__init__()  # init base\n",
    "        self.ht = ht  # expected grid height\n",
    "        self.wt = wt  # expected grid width\n",
    "        self.ln1 = nn.LayerNorm(dim)  # LN before attention\n",
    "        self.attn = FlexibleMHSA(dim, heads, attn_dropout=attn_dropout, proj_dropout=dropout)  # MHSA\n",
    "        self.ln2 = nn.LayerNorm(dim)  # LN before DWConv\n",
    "        self.dwconv = nn.Conv2d(dim, dim, kernel_size=3, padding=1, groups=dim, bias=True)  # depth-wise conv\n",
    "        self.dw_drop = nn.Dropout(dropout)  # dropout\n",
    "        self.ln3 = nn.LayerNorm(dim)  # LN before MLP\n",
    "        self.mlp = nn.Sequential(  # MLP\n",
    "            nn.Linear(dim, mlp_dim),  # expand\n",
    "            nn.GELU(),  # activate\n",
    "            nn.Dropout(dropout),  # drop\n",
    "            nn.Linear(mlp_dim, dim),  # project back\n",
    "            nn.Dropout(dropout),  # drop\n",
    "        )  # end MLP\n",
    "    def forward(self, x, return_attn=False):  # x: (B,N,D)\n",
    "        attn_out, attn = self.attn(self.ln1(x), return_attn=return_attn)  # attention\n",
    "        x = x + attn_out  # residual add\n",
    "        y = self.ln2(x)  # normalize\n",
    "        B, N, D = y.shape  # unpack\n",
    "        ht, wt = self.ht, self.wt  # configured grid\n",
    "        if ht * wt != N:  # if mismatch\n",
    "            side = int(math.sqrt(N))  # infer side\n",
    "            ht = max(side, 1)  # clamp\n",
    "            wt = max(N // ht, 1)  # clamp\n",
    "        y2 = y.transpose(1, 2).reshape(B, D, ht, wt)  # tokens -> grid\n",
    "        y2 = self.dwconv(y2)  # local mixing\n",
    "        y2 = y2.reshape(B, D, ht * wt).transpose(1, 2)  # grid -> tokens\n",
    "        y2 = self.dw_drop(y2)  # dropout\n",
    "        x = x + y2  # residual add\n",
    "        x = x + self.mlp(self.ln3(x))  # MLP residual add\n",
    "        return x, attn  # return tokens + attention\n",
    "\n",
    "class RViTEncoder(nn.Module):  # stacked encoder\n",
    "    def __init__(self, dim=142, depth=10, heads=10, mlp_dim=480, attn_dropout=0.1, dropout=0.1, ht=14, wt=14):  # config\n",
    "        super().__init__()  # init\n",
    "        self.blocks = nn.ModuleList([  # blocks list\n",
    "            RViTBlock(dim, heads, mlp_dim, attn_dropout=attn_dropout, dropout=dropout, ht=ht, wt=wt)  # block init\n",
    "            for _ in range(depth)  # repeat\n",
    "        ])  # end list\n",
    "        self.ln = nn.LayerNorm(dim)  # final LN\n",
    "    def forward(self, x, return_attn=False):  # x: (B,N,D)\n",
    "        attn_list = []  # collect attention maps\n",
    "        for blk in self.blocks:  # loop blocks\n",
    "            x, attn = blk(x, return_attn=return_attn)  # forward\n",
    "            if return_attn and attn is not None:  # collect if requested\n",
    "                attn_list.append(attn)  # store\n",
    "        x = self.ln(x)  # final LN\n",
    "        return x, attn_list  # output tokens + attention list\n",
    "\n",
    "# -------------------------  # section divider\n",
    "# Hybrid model (FULL or ABLATION via flag)  # main classifier class\n",
    "# -------------------------  # section divider\n",
    "class HybridResNet50V2_RViT(nn.Module):  # PFDB-GSTEB model\n",
    "    def __init__(  # constructor\n",
    "        self,  # self\n",
    "        num_classes=4,  # multiclass output\n",
    "        img_size=224,  # input size\n",
    "        patch_size=16,  # ViT patch size\n",
    "        embed_dim=142,  # token dim\n",
    "        depth=10,  # encoder depth\n",
    "        heads=10,  # heads\n",
    "        mlp_dim=480,  # MLP dim\n",
    "        attn_dropout=0.1,  # attn dropout\n",
    "        vit_dropout=0.1,  # block dropout\n",
    "        fusion_dim=256,  # fusion dim\n",
    "        fusion_dropout=0.5,  # fusion dropout\n",
    "        rotations=(0, 1, 2, 3),  # rot set\n",
    "        cnn_name=\"resnetv2_50x1_bitm\",  # backbone id\n",
    "        cnn_pretrained=True,  # pretrained weights\n",
    "        use_pfd_gste=True,  # toggle ablation\n",
    "        gste_min_side=7,  # minimum dynamic side\n",
    "        gste_std_flat=0.05,  # flat-mask threshold\n",
    "        gste_std_full=0.30,  # full-shrink threshold\n",
    "        gste_max_shrink=0.50,  # max shrink fraction\n",
    "    ):  # end signature\n",
    "        super().__init__()  # init base\n",
    "        self.num_classes = num_classes  # store\n",
    "        self.rotations = rotations  # store\n",
    "        self.patch_size = patch_size  # store\n",
    "        self.img_size = img_size  # store\n",
    "        self.use_pfd_gste = bool(use_pfd_gste)  # store as bool\n",
    "\n",
    "        # CNN branch (Sarada starting point: pretrained backbone)  # backbone path\n",
    "        self.cnn = ResNet50V2TimmBackbone(model_name=cnn_name, pretrained=cnn_pretrained)  # init CNN\n",
    "        self.cnn_out_ch = int(self.cnn.out_ch)  # read output channels\n",
    "\n",
    "        # Krsna PFD  # my gating module\n",
    "        self.pfd = PFD(in_ch=self.cnn_out_ch)  # init PFD\n",
    "\n",
    "        # Sarada-style head regularisation (minimal, backbone intact)  # regularize pooled descriptor\n",
    "        self.cnn_drop = nn.Dropout(p=float(fusion_dropout))  # dropout on pooled CNN vector\n",
    "        self.cnn_proj = nn.Linear(self.cnn_out_ch, fusion_dim)  # project to fusion dim\n",
    "        self.cnn_bn = nn.BatchNorm1d(fusion_dim)  # BN on projected CNN vector\n",
    "        self.cnn_pool = nn.AdaptiveAvgPool2d(1)  # global average pool\n",
    "\n",
    "        # Transformer branch: Krishnan patchify IMAGE  # raw image -> tokens path\n",
    "        grid = img_size // patch_size  # 14 for 224/16\n",
    "        self.base_grid = int(grid)  # store base grid\n",
    "        self.gste_min_side = int(max(1, min(gste_min_side, self.base_grid)))  # clamp min side\n",
    "\n",
    "        # GSTE selection knobs (robust dynamic side selection)  # hyperparameters\n",
    "        self.gste_std_flat = float(gste_std_flat)  # below => keep full grid\n",
    "        self.gste_std_full = float(gste_std_full)  # above => allow max shrink\n",
    "        self.gste_max_shrink = float(gste_max_shrink)  # shrink fraction cap\n",
    "\n",
    "        self.patch = ImagePatchEmbed(img_size=img_size, patch_size=patch_size, in_chans=3, embed_dim=embed_dim)  # patch embed\n",
    "        self.posrot = PositionalAndRotationEmbedding(base_h=grid, base_w=grid, embed_dim=embed_dim, n_rot=4)  # pos+rot embed\n",
    "        self.encoder = RViTEncoder(  # encoder\n",
    "            dim=embed_dim,  # dim\n",
    "            depth=depth,  # depth\n",
    "            heads=heads,  # heads\n",
    "            mlp_dim=mlp_dim,  # mlp dim\n",
    "            attn_dropout=attn_dropout,  # attn dropout\n",
    "            dropout=vit_dropout,  # dropout\n",
    "            ht=grid,  # expected ht\n",
    "            wt=grid,  # expected wt\n",
    "        )  # end encoder\n",
    "        self.vit_proj = nn.Linear(embed_dim, fusion_dim)  # transformer pooled -> fusion dim\n",
    "\n",
    "        # Fusion head  # combine CNN and transformer\n",
    "        self.fuse_fc = nn.Linear(fusion_dim * 2, fusion_dim)  # concat -> fusion\n",
    "        self.fuse_drop = nn.Dropout(p=float(fusion_dropout))  # dropout\n",
    "        self.out = nn.Linear(fusion_dim, num_classes)  # logits\n",
    "\n",
    "        # For external XAI hooks (optional)  # saved tensors for Grad-CAM++ etc.\n",
    "        self._last_cnn_feat = None  # stores last CNN feature map used\n",
    "        self._last_cnn_mask_img = None  # stores last upsampled mask\n",
    "\n",
    "    def freeze_cnn_bn(self):  # freeze BN layers in CNN\n",
    "        for m in self.cnn.modules():  # iterate CNN modules\n",
    "            if isinstance(m, nn.BatchNorm2d):  # BN2d layers\n",
    "                m.eval()  # set eval\n",
    "\n",
    "    def _mask_to_alpha_grid(self, mask_img, out_side):  # convert image-scale mask to alpha grid\n",
    "        \"\"\" mask_img: (B,1,H,W) -> adaptive pool to (out_side,out_side) Return alpha_grid with meanâ‰ˆ1 per-sample to keep scale stable. \"\"\"  # doc\n",
    "        alpha = F.adaptive_avg_pool2d(mask_img, output_size=(out_side, out_side))  # (B,1,s,s)\n",
    "        alpha = alpha / (alpha.mean(dim=(2, 3), keepdim=True) + 1e-6)  # normalize meanâ‰ˆ1\n",
    "        return alpha  # return grid weights\n",
    "\n",
    "    def _choose_dynamic_side(self, alpha_base):  # decide dynamic grid size\n",
    "        \"\"\" GSTE dynamic grid side in [gste_min_side .. base_grid], guided by mask concentration.\n",
    "        Fixes the \"collapse on flat mask\" issue:\n",
    "        - If mask is flat/uncertain (std very low), KEEP full grid.\n",
    "        - Shrink grid only when mask is truly concentrated/peaked.\n",
    "        \"\"\"  # policy docstring\n",
    "        with torch.no_grad():  # no gradients for side decision\n",
    "            std = alpha_base.std(dim=(2, 3), keepdim=False)  # (B,1) std across grid\n",
    "            s = float(std.mean().item())  # scalar proxy for concentration\n",
    "            if s <= self.gste_std_flat:  # flat mask => no shrink\n",
    "                return self.base_grid  # keep 14\n",
    "            denom = max(self.gste_std_full - self.gste_std_flat, 1e-6)  # avoid divide by zero\n",
    "            t = (s - self.gste_std_flat) / denom  # normalize into [0,1] range\n",
    "            t = max(0.0, min(1.0, t))  # clamp\n",
    "            shrink = self.gste_max_shrink * t  # shrink fraction\n",
    "            target = int(round(self.base_grid * (1.0 - shrink)))  # target side\n",
    "            side = max(self.gste_min_side, min(self.base_grid, target))  # clamp to allowed\n",
    "            return side  # return side\n",
    "\n",
    "    def _gste_dynamic_tokens(self, tokens_base, alpha_base, side):  # dynamic token evolution\n",
    "        \"\"\" Mask-guided dynamic token evolution:\n",
    "        - tokens_base: (B,N,D) with N=base_grid^2\n",
    "        - alpha_base : (B,1,base_grid,base_grid), meanâ‰ˆ1\n",
    "        - side: dynamic grid side (<= base_grid)\n",
    "        Returns tokens_dyn: (B, side*side, D) and (side,side).\n",
    "        \"\"\"  # doc\n",
    "        B, N, D = tokens_base.shape  # unpack\n",
    "        g = self.base_grid  # base side\n",
    "        x = tokens_base.transpose(1, 2).reshape(B, D, g, g)  # tokens -> (B,D,g,g)\n",
    "        w = alpha_base  # (B,1,g,g) weights\n",
    "        x = x * w  # apply ROI weighting (broadcast)\n",
    "        if side < g:  # if downsample needed\n",
    "            num = F.adaptive_avg_pool2d(x, output_size=(side, side))  # weighted numerator\n",
    "            den = F.adaptive_avg_pool2d(w, output_size=(side, side)) + 1e-6  # weight sum\n",
    "            x = num / den  # weighted average pooling\n",
    "        tokens = x.flatten(2).transpose(1, 2)  # back to (B,side*side,D)\n",
    "        return tokens, side, side  # return tokens + grid\n",
    "\n",
    "    def forward(self, x, return_xai=False):  # forward\n",
    "        # =========================  # section divider\n",
    "        # CNN backbone  # compute CNN features\n",
    "        # =========================  # section divider\n",
    "        feat = self.cnn(x)  # (B,C,7,7)\n",
    "\n",
    "        # =========================  # section divider\n",
    "        # PFD (optional)  # compute mask + gated features\n",
    "        # =========================  # section divider\n",
    "        if self.use_pfd_gste:  # full model\n",
    "            feat_path, mask_feat = self.pfd(feat)  # gated feat + mask\n",
    "            feat_for_cnn = feat_path  # PFDB: also gate the CNN descriptor path\n",
    "            mask_img = F.interpolate(mask_feat, size=(x.shape[2], x.shape[3]), mode=\"bilinear\", align_corners=False)  # mask at image scale\n",
    "        else:  # ablation path\n",
    "            feat_for_cnn = feat  # no gating\n",
    "            mask_img = None  # no mask\n",
    "\n",
    "        # Save for possible Grad-CAM++ hooking / debugging  # XAI support\n",
    "        self._last_cnn_feat = feat_for_cnn  # stash feature map\n",
    "        self._last_cnn_mask_img = mask_img  # stash mask\n",
    "\n",
    "        # =========================  # section divider\n",
    "        # CNN pooled vector (Sarada-style head regularisation)  # pooled descriptor branch\n",
    "        # =========================  # section divider\n",
    "        z_cnn = self.cnn_pool(feat_for_cnn).flatten(1)  # (B,C)\n",
    "        z_cnn = self.cnn_drop(z_cnn)  # dropout\n",
    "        z_cnn = self.cnn_proj(z_cnn)  # (B,fusion_dim)\n",
    "        z_cnn = self.cnn_bn(z_cnn)  # BN\n",
    "        z_cnn = F.relu(z_cnn, inplace=True)  # activation\n",
    "\n",
    "        # =========================  # section divider\n",
    "        # GSTE: choose dynamic token grid once (batch-consistent) from mask  # dynamic grid decision\n",
    "        # =========================  # section divider\n",
    "        if self.use_pfd_gste:  # if mask exists\n",
    "            alpha0 = self._mask_to_alpha_grid(mask_img, self.base_grid)  # (B,1,14,14)\n",
    "            dyn_side = self._choose_dynamic_side(alpha0)  # choose side\n",
    "        else:  # no GSTE\n",
    "            dyn_side = self.base_grid  # keep 14\n",
    "\n",
    "        # =========================  # section divider\n",
    "        # Krishnan RViT rotations: rotate IMAGE, patchify P=16, average embeddings  # core RViT path\n",
    "        # =========================  # section divider\n",
    "        token_sets = []  # collect per-rotation token sequences\n",
    "        for k in self.rotations:  # loop rotations\n",
    "            rot_id = int(k) % 4  # rotation id in 0..3\n",
    "            x_r = torch.rot90(x, k=rot_id, dims=(2, 3))  # rotate the raw image\n",
    "            tokens_base, ht, wt = self.patch(x_r)  # patchify -> (B,196,D), ht=wt=14\n",
    "            if self.use_pfd_gste:  # apply mask guidance\n",
    "                m_r = torch.rot90(mask_img, k=rot_id, dims=(2, 3))  # rotate mask with image\n",
    "                alpha_base = self._mask_to_alpha_grid(m_r, self.base_grid)  # (B,1,14,14)\n",
    "                tokens, ht, wt = self._gste_dynamic_tokens(tokens_base, alpha_base, dyn_side)  # dynamic grid + weighting\n",
    "            else:  # ablation\n",
    "                tokens, ht, wt = tokens_base, ht, wt  # keep base tokens\n",
    "            tokens = self.posrot(tokens, ht, wt, rot_id)  # add pos + rot embedding (interpolates if needed)\n",
    "            token_sets.append(tokens)  # store tokens\n",
    "\n",
    "        Tavg = torch.stack(token_sets, dim=0).mean(dim=0)  # average over rotations before encoder\n",
    "\n",
    "        # =========================  # section divider\n",
    "        # Transformer encoder (MHSA + DWConv + MLP)  # global reasoning\n",
    "        # =========================  # section divider\n",
    "        Tenc, attn_list = self.encoder(Tavg, return_attn=return_xai)  # encode tokens\n",
    "        z_vit = Tenc.mean(dim=1)  # global average pool tokens\n",
    "        z_vit = F.relu(self.vit_proj(z_vit), inplace=True)  # project -> fusion_dim\n",
    "\n",
    "        # =========================  # section divider\n",
    "        # Fusion + classifier  # combine branches\n",
    "        # =========================  # section divider\n",
    "        z = torch.cat([z_cnn, z_vit], dim=1)  # concat\n",
    "        h = F.relu(self.fuse_fc(z), inplace=True)  # fuse\n",
    "        h = self.fuse_drop(h)  # dropout\n",
    "        logits = self.out(h)  # logits\n",
    "\n",
    "        if return_xai:  # return explanation payload\n",
    "            return logits, {  # package dict\n",
    "                \"attn\": attn_list,  # transformer attentions\n",
    "                \"mask\": mask_img,  # image-scale mask\n",
    "                \"gste_side\": dyn_side,  # dynamic grid size used\n",
    "            }  # end dict\n",
    "        return logits, None  # default path\n",
    "\n",
    "    @torch.no_grad()  # inference-only\n",
    "    def mc_dropout_predict(self, x, mc_samples=20):  # MC dropout\n",
    "        self.eval()  # eval mode\n",
    "        for m in self.modules():  # iterate modules\n",
    "            if isinstance(m, (nn.Dropout, nn.Dropout2d)):  # dropout layers\n",
    "                m.train()  # enable dropout\n",
    "        probs = []  # collect probs\n",
    "        for _ in range(int(mc_samples)):  # repeat\n",
    "            logits, _ = self.forward(x, return_xai=False)  # forward\n",
    "            probs.append(torch.softmax(logits, dim=1))  # softmax\n",
    "        probs = torch.stack(probs, dim=0)  # stack samples\n",
    "        mu = probs.mean(dim=0)  # mean\n",
    "        var = probs.var(dim=0, unbiased=False)  # variance\n",
    "        return mu, var  # return\n",
    "\n",
    "class HybridResNet50V2_RViT_Ablation(HybridResNet50V2_RViT):  # ablation wrapper\n",
    "    \"\"\" Ablation: Hybrid WITHOUT Krsna extensions (PFD + GSTE). Still paper-faithful to Krishnan RViT (rotate IMAGE, patchify P=16, avg embeddings). \"\"\"  # what this ablation does\n",
    "    def __init__(self, *args, **kwargs):  # pass-through init\n",
    "        kwargs[\"use_pfd_gste\"] = False  # force ablation\n",
    "        super().__init__(*args, **kwargs)  # call parent init\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470f4532",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m py_compile models/hybrid_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615d53bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/dataset_prep.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33564b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls data/splits/tightcrop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241befbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/train.py \\\n",
    "    --csv_dir data/splits/tightcrop \\\n",
    "    --out_dir results/run_hybrid_with__pfdB_gsteB\\\n",
    "    --epochs 100 \\\n",
    "    --batch_size 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9686e9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls results/run_hybrid_with_no_pfd_gste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87106806",
   "metadata": {},
   "outputs": [],
   "source": [
    "!PYTHONPATH=/kaggle/working python scripts/predict_xai.py \\\n",
    "  --checkpoint results/run_hybrid_with_pfdB_gsteB/best_model.pt \\\n",
    "  --image \"/kaggle/working/data/processed/tightcrop/test/pituitary/cec55b0d841f09f09f21874b96c667b5d2b06b4a.jpg\" \\\n",
    "  --out_dir results/xai\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f63bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for p in [\"results/xai/xai_gradcampp.png\", \"results/xai/xai_attn_rollout.png\"]:\n",
    "    img = Image.open(p)\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.imshow(img)\n",
    "    plt.title(p)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
